{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#OLS\" data-toc-modified-id=\"OLS-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>OLS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unpooled\" data-toc-modified-id=\"Unpooled-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Unpooled</a></span><ul class=\"toc-item\"><li><span><a href=\"#All-logs\" data-toc-modified-id=\"All-logs-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>All logs</a></span></li><li><span><a href=\"#Logs-or-squares\" data-toc-modified-id=\"Logs-or-squares-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Logs or squares</a></span></li></ul></li><li><span><a href=\"#Pooled\" data-toc-modified-id=\"Pooled-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pooled</a></span></li></ul></li><li><span><a href=\"#Linear-model-with-elastic-net-regularization\" data-toc-modified-id=\"Linear-model-with-elastic-net-regularization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linear model with elastic net regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unpooled\" data-toc-modified-id=\"Unpooled-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Unpooled</a></span></li><li><span><a href=\"#Pooled\" data-toc-modified-id=\"Pooled-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pooled</a></span></li></ul></li><li><span><a href=\"#Save-performance\" data-toc-modified-id=\"Save-performance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Save performance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "# import pickle\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    StratifiedShuffleSplit, cross_val_score, KFold\n",
    "#     GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, \\\n",
    "    ElasticNet\n",
    "#     LogisticRegressionCV, SGDClassifier\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, \\\n",
    "#     ExtraTreesClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "# from sklearn.utils.fixes import signature\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval, pyll\n",
    "# import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# # Load line profiler\n",
    "# %load_ext line_profiler\n",
    "\n",
    "# Adjust number of CPU cores to use\n",
    "N_JOBS=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we trained a gradient boosting model, in order to have the flexibility to model non-linear relationships and interactions between the predictors. Gradient boosting, and in particular the version I used, XGBoost, is known to deliver some of the most accurate predictions.  In fact, on anything short of huge data sets with at least millions of observations, where deep learning is likely to perform best, it is probably the most accurate individual model. \n",
    "\n",
    "However, in practice linear regression is often the method of choice, because of its simplicity as well as its better interpretability. Thus, letâ€™s see for comparison what accuracy we achieve using OLS. After that, I will add regularization to these models to avoid overfitting the model to the particular data we have at hand.  This will show that, if we want to include many parameters in the model (e.g., to take into account the effect of variables like the different metropolitan areas), regularization is important in order to get good performance on new data.\n",
    "By the same token, this also shows that we need to rethink how we evaluate the performance of a model: If we want to use the explained variance, it is not enough to simply use the adjusted R_2, since this adjustment is not done in any principled way.  Rather, we need to withhold data when training the model, and then evaluate the performance of the model (e.g., by calculating the R_2) on these withheld data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data from the previous note book and perform the specific preprocessing steps necessary for linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = joblib.load('../data/data_train_lin')\n",
    "data_test = joblib.load('../data/data_test_lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will encode year using one-hot encoding (i.e., using dummy variables), we need to deal with the fact that the test set has years not found in the training set. The best solution seems to be to set the new years to the last year for which we have data.  Our training set goes to mid-2016, gives and a test set goes from mid-2016 to 2017.Therefore, we simply said all years in the test set to 2016, which should give us the best possible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set year for test set to the most recent year\n",
    "data_test.year = 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpooled\n",
    "#### All logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will perform one-hot encoding for categorical variables. By contrast to the previous notebook, we will also treat the ordinal variable, hotel category, as categorical. This is necessary because linear models would otherwise wrongly treat the variable as measured on an interval-scale, which is unlikely to be appropriate (this would assume that the difference between each succeeding pair of categories is identical in size).\n",
    "\n",
    "For the models without regularization, we need to drop the first category of each categorical variable in order to avoid collinearity. This is not so easy to achieve with the python data science stack, because we normally would not want to estimate a model without regularization. Pandas.get_dummies() does offer this possibility, but it then does not give us the option to transform the training data using the same encoding. Thus, the best option is to use scikit-learn's one-hot encoder on each categorical column individually. I will store these in a list, and then drop the first column before concatenating these new data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['msa', 'tract', 'location', 'category',\n",
    "        'saleaffiliation', 'operation', 'year', 'quarter']\n",
    "# Instantiate one-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "# Create lists to store encodings for each variable\n",
    "X_train_cats = []\n",
    "X_test_cats = []\n",
    "feature_names_cats = []\n",
    "# Iterate through categorical variables\n",
    "for cat in cats:\n",
    "    # Encode variable i for TEST data\n",
    "    ohe_train_i = ohe.fit_transform(\n",
    "                data_train.loc[:, [cat]])\n",
    "    # Encode variable i for TRAIN data\n",
    "    ohe_test_i = ohe.transform(\n",
    "                data_test.loc[:, [cat]])\n",
    "    \n",
    "    # Discard first column and save\n",
    "    X_train_cats.append(ohe_train_i[:,1:])\n",
    "    X_test_cats.append(ohe_test_i[:,1:])\n",
    "    \n",
    "    # Save feature names (w/o first column)\n",
    "    feature_names_cats.extend(\n",
    "        ohe.get_feature_names()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables excluded:  ['year_2', 'year_3', 'age_2', 'age_3', 'floors_2', 'landsf', 'age', 'largestmeetingspace', 'sizesf', 'rooms', 'floors', 'saleprice', 'log_saleprice', 'id', 'new']\n"
     ]
    }
   ],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "        data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train the model.  Even though I removed the first category of each categorical variable for which I used one-hot-encoding, I also had to remove the intercept in order to get a reasonable prediction on the test set. (With the intercept, I got a negative R^2.)  Even though statisticians have developed tools to dig deeper into collinearity to find out which variables are causing the problem, I don't waste any time on this. As explained above, there is no reason to estimate a model without regularization anyway, except for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr= LinearRegression(fit_intercept=False)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row0\" class=\"row_heading level0 row0\" >allsuites</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row0_col0\" class=\"data row0 col0\" >-0.01566</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row1\" class=\"row_heading level0 row1\" >boutique</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row1_col0\" class=\"data row1 col0\" >-0.03987</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row2\" class=\"row_heading level0 row2\" >casino</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row2_col0\" class=\"data row2 col0\" >0.74781</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row3\" class=\"row_heading level0 row3\" >cbd</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row3_col0\" class=\"data row3 col0\" >0.72929</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row4\" class=\"row_heading level0 row4\" >conference</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row4_col0\" class=\"data row4 col0\" >0.22607</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row5\" class=\"row_heading level0 row5\" >convention</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row5_col0\" class=\"data row5 col0\" >0.32923</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row6\" class=\"row_heading level0 row6\" >golf</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row6_col0\" class=\"data row6 col0\" >0.10225</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row7\" class=\"row_heading level0 row7\" >indoorcorridors</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row7_col0\" class=\"data row7 col0\" >0.01384</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row8\" class=\"row_heading level0 row8\" >log1_age</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row8_col0\" class=\"data row8 col0\" >-0.11058</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row9\" class=\"row_heading level0 row9\" >log1_landsf</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row9_col0\" class=\"data row9 col0\" >0.00363</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row10\" class=\"row_heading level0 row10\" >log1_largestmeetingspace</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row10_col0\" class=\"data row10 col0\" >0.00154</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row11\" class=\"row_heading level0 row11\" >log_floors</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row11_col0\" class=\"data row11 col0\" >0.10042</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row12\" class=\"row_heading level0 row12\" >log_rooms</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row12_col0\" class=\"data row12 col0\" >0.44248</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row13\" class=\"row_heading level0 row13\" >log_sizesf</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row13_col0\" class=\"data row13 col0\" >0.18693</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row14\" class=\"row_heading level0 row14\" >multiproperty</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row14_col0\" class=\"data row14 col0\" >-0.04945</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row15\" class=\"row_heading level0 row15\" >portfolio</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row15_col0\" class=\"data row15 col0\" >0.20153</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row16\" class=\"row_heading level0 row16\" >restaurant</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row16_col0\" class=\"data row16 col0\" >-0.00400</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row17\" class=\"row_heading level0 row17\" >selfrun</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row17_col0\" class=\"data row17 col0\" >-0.07715</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row18\" class=\"row_heading level0 row18\" >ski</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row18_col0\" class=\"data row18 col0\" >0.25684</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969level0_row19\" class=\"row_heading level0 row19\" >spa</th> \n",
       "        <td id=\"T_f932a84a_3dfb_11e9_8472_28d24441a969row19_col0\" class=\"data row19 col0\" >0.14897</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x29696f1a358>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8567275967358019"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5850465400933892"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait with interpreting these results until the next notebook, where we can compare the results from all models.  Thus, we will store these results for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8567275967358019, 0.5850465400933892]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_up = [r2_lr_train, r2_lr]\n",
    "r2_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs or squares\n",
    "Now let's estimate the same type of model but with different predictors: Instead of using all logs for numeric variables, I will follow Das et al (2017) and instead use a square term for age and floors, as well as a the levels (rather than log) for the amount of land included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2include = \\\n",
    "        ['allsuites', 'boutique', 'casino', 'cbd', \n",
    "         'conference', 'convention', 'golf', 'indoorcorridors',\n",
    "         'multiproperty', 'portfolio', 'restaurant', \n",
    "         'ski', 'spa', 'landsf', 'age', 'age_2',\n",
    "         'log1_largestmeetingspace', 'log_sizesf', 'log_rooms', \n",
    "         'floors', 'floors_2', 'selfrun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.loc[:, v2include]\n",
    "X_test_rest= data_test.loc[:, v2include]\n",
    "\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr= LinearRegression(fit_intercept=False)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row0\" class=\"row_heading level0 row0\" >age</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row0_col0\" class=\"data row0 col0\" >-0.00737</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row1\" class=\"row_heading level0 row1\" >age_2</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row1_col0\" class=\"data row1 col0\" >0.00004</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row2\" class=\"row_heading level0 row2\" >allsuites</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row2_col0\" class=\"data row2 col0\" >-0.01015</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row3\" class=\"row_heading level0 row3\" >boutique</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row3_col0\" class=\"data row3 col0\" >-0.04466</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row4\" class=\"row_heading level0 row4\" >casino</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row4_col0\" class=\"data row4 col0\" >0.73540</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row5\" class=\"row_heading level0 row5\" >cbd</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row5_col0\" class=\"data row5 col0\" >0.69365</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row6\" class=\"row_heading level0 row6\" >conference</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row6_col0\" class=\"data row6 col0\" >0.22503</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row7\" class=\"row_heading level0 row7\" >convention</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row7_col0\" class=\"data row7 col0\" >0.31264</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row8\" class=\"row_heading level0 row8\" >floors</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row8_col0\" class=\"data row8 col0\" >0.02832</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row9\" class=\"row_heading level0 row9\" >floors_2</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row9_col0\" class=\"data row9 col0\" >-0.00051</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row10\" class=\"row_heading level0 row10\" >golf</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row10_col0\" class=\"data row10 col0\" >0.06079</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row11\" class=\"row_heading level0 row11\" >indoorcorridors</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row11_col0\" class=\"data row11 col0\" >0.01657</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row12\" class=\"row_heading level0 row12\" >landsf</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row12_col0\" class=\"data row12 col0\" >0.00000</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row13\" class=\"row_heading level0 row13\" >log1_largestmeetingspace</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row13_col0\" class=\"data row13 col0\" >0.00149</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row14\" class=\"row_heading level0 row14\" >log_rooms</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row14_col0\" class=\"data row14 col0\" >0.43780</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row15\" class=\"row_heading level0 row15\" >log_sizesf</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row15_col0\" class=\"data row15 col0\" >0.19471</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row16\" class=\"row_heading level0 row16\" >multiproperty</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row16_col0\" class=\"data row16 col0\" >-0.05327</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row17\" class=\"row_heading level0 row17\" >portfolio</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row17_col0\" class=\"data row17 col0\" >0.19091</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row18\" class=\"row_heading level0 row18\" >restaurant</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row18_col0\" class=\"data row18 col0\" >-0.00211</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row19\" class=\"row_heading level0 row19\" >selfrun</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row19_col0\" class=\"data row19 col0\" >-0.08168</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row20\" class=\"row_heading level0 row20\" >ski</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row20_col0\" class=\"data row20 col0\" >0.25838</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969level0_row21\" class=\"row_heading level0 row21\" >spa</th> \n",
       "        <td id=\"T_444c1f08_3dfc_11e9_8769_28d24441a969row21_col0\" class=\"data row21 col0\" >0.14042</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2969734dda0>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8565506116601053"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5890885744341479"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's print the results from the model using all logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8567275967358019, 0.5850465400933892]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 for model with all logs \n",
    "# (for training and test set, respectively)\n",
    "r2_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the two models performed very similarly, especially on the training set. On the test set, the model with polynomial terms performs slightly better than the model with all logs.  However, the difference is likely within the margin of error. (Indeed, when I tried a different specification that used a linear time trend, I got the reverse result.)  Thus, I will stick with the model with all logs for now, since this is more robust to outliers: As already mentioned, polynomials can give predictions that are far off for observations that have variables with extreme values, such as very old buildings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to estimate the pooled model without regularization.  To do so, we first repeat the preprocessing steps but do not perform one-hot encoding for the three categorical variables that have a large number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['location', 'category', 'operation', \n",
    "        'year', 'quarter']\n",
    "# Categorical variables to exclude\n",
    "cats_out = ['msa', 'tract', 'saleaffiliation']\n",
    "# Instantiate one-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "# Create lists to store encodings for each variable\n",
    "X_train_cats = []\n",
    "X_test_cats = []\n",
    "feature_names_cats = []\n",
    "for cat in cats:\n",
    "    # Encode variable i for TEST data\n",
    "    ohe_train_i = ohe.fit_transform(\n",
    "                data_train.loc[:, [cat]])\n",
    "    # Encode variable i for TRAIN data\n",
    "    ohe_test_i = ohe.transform(\n",
    "                data_test.loc[:, [cat]])\n",
    "    \n",
    "    # Discard first column and save\n",
    "    X_train_cats.append(ohe_train_i[:,1:])\n",
    "    X_test_cats.append(ohe_test_i[:,1:])\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_cats.extend(\n",
    "        ohe.get_feature_names()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables excluded:  ['year_2', 'year_3', 'age_2', 'age_3', 'floors_2', 'landsf', 'age', 'largestmeetingspace', 'sizesf', 'rooms', 'floors', 'saleprice', 'log_saleprice', 'id', 'new']\n"
     ]
    }
   ],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "        data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + cats_out + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + cats_out + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready again to estimate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x0_Luxury Class             1.666332\n",
       "x0_Upper Upscale Class      1.184319\n",
       "x0_Upscale Class            0.898120\n",
       "casino                      0.696283\n",
       "x0_Upper Midscale Class     0.554321\n",
       "log_rooms                   0.467119\n",
       "x0_Urban                    0.315730\n",
       "log_floors                  0.290715\n",
       "convention                  0.281182\n",
       "spa                         0.270852\n",
       "portfolio                   0.254066\n",
       "x0_Midscale Class           0.235429\n",
       "conference                  0.230456\n",
       "x0_Resort                   0.224743\n",
       "golf                        0.135052\n",
       "boutique                    0.130072\n",
       "log_sizesf                  0.081230\n",
       "restaurant                  0.042378\n",
       "x0_4                        0.036075\n",
       "allsuites                   0.028218\n",
       "multiproperty               0.026801\n",
       "x0_3                        0.018496\n",
       "log1_largestmeetingspace   -0.013087\n",
       "indoorcorridors            -0.020416\n",
       "x0_2                       -0.025224\n",
       "log1_landsf                -0.028681\n",
       "cbd                        -0.040998\n",
       "x0_Suburban                -0.049544\n",
       "selfrun                    -0.097275\n",
       "ski                        -0.100907\n",
       "log1_age                   -0.122570\n",
       "x0_Franchise               -0.154934\n",
       "x0_Small Metro/Town        -0.195506\n",
       "x0_Independent             -0.204187\n",
       "x0_Interstate              -0.205399\n",
       "x0_2000                    -0.477976\n",
       "x0_2016                    -0.502132\n",
       "x0_2007                    -0.585350\n",
       "x0_2004                    -0.631789\n",
       "x0_2015                    -0.643029\n",
       "x0_1998                    -0.647251\n",
       "x0_2008                    -0.659997\n",
       "x0_2003                    -0.743510\n",
       "x0_2014                    -0.746934\n",
       "x0_1993                    -0.747698\n",
       "x0_2005                    -0.752032\n",
       "x0_2001                    -0.757654\n",
       "x0_2006                    -0.759520\n",
       "x0_2013                    -0.873526\n",
       "x0_2009                    -0.910439\n",
       "x0_2010                    -0.952261\n",
       "x0_2012                    -0.957405\n",
       "x0_2011                    -0.957732\n",
       "x0_2002                    -1.603999\n",
       "x0_1996                    -2.009283\n",
       "x0_1994                    -2.040058\n",
       "dtype: float64"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the explained variance for training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.722600681359757"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810508365295534"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.722600681359757, 0.7810508365295534]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save performance\n",
    "r2_p = [r2_lr_train, r2_lr]\n",
    "r2_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the model does a little better on the test set. On average, we would expect it to do worse. However, there is variance to our estimated performance on training and test set.  In our case, this variance is increased by the fact that we don't have a huge sample size (the test set contains about 1300 observations), and that we are using a single validation set rather than cross-validation in order to take into account the time-series nature of the data. Even more importantly, the training and test set do not come from the same distribution, because we split the data based on time.  It seems that the newer data were easier to predict, maybe because we have more data for the intercept of year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model with elastic net regularization\n",
    "### Unpooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the preprocessing steps again.  By contrast to the models without regularization, we do not get rid of the first category for each categorical variables.  The reason this is not necessary is that regularization is an alternative way to avoid collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['msa', 'tract', 'location', 'category',\n",
    "        'saleaffiliation', 'operation', 'year',\n",
    "        'quarter']\n",
    "# Perform one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "X_train_cats = ohe.fit_transform(\n",
    "    data_train.loc[:, cats])        \n",
    "X_test_cats = ohe.transform(\n",
    "    data_test.loc[:, cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables excluded:  ['year_2', 'year_3', 'age_2', 'age_3', 'floors_2', 'landsf', 'age', 'largestmeetingspace', 'sizesf', 'rooms', 'floors', 'saleprice', 'log_saleprice', 'id', 'new']\n"
     ]
    }
   ],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "    data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train_up = np.concatenate([X_train_rest, X_train_cats], \n",
    "                         axis=1)\n",
    "X_test_up = np.concatenate([X_test_rest, X_test_cats], \n",
    "                         axis=1)\n",
    "# X_train_up = X_train_up.drop(\n",
    "#     v2exclude, axis=1)\n",
    "# X_test_up = X_test_up.drop(\n",
    "#     v2exclude, axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + list(ohe.get_feature_names())\n",
    "\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no go ahead and train the model.  Like for gradient boosting, will use Hyperopt to perform Bayesian hyperparameters optimization.  It probably does not lead to any big increase in performance or decrease in computational cost compared to a brute-force search, because computational costs for a linear model are relatively low anyways for a data set of our size. But since it is not too much of a burden to adapt the  code from XGBoost, we might as well reuse it just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(LEARNER, space, model_name, \n",
    "                 X_train, y_train, \n",
    "                 adjust_params=None,n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given regressor and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress (not currently used)\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = '../progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, LEARNER=LEARNER, \n",
    "                  progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "        \n",
    "        # Adjust parameters, if specified\n",
    "        if adjust_params is not None:\n",
    "            params = adjust_params(params)\n",
    "    \n",
    "        # Instantiate LEARNER\n",
    "        learner = LEARNER(**params)\n",
    "        \n",
    "        ## Generate indices for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = ShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = KFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(learner, X_train, y_train, cv=cv,\n",
    "                                scoring='r2', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "    # Adjust best parameters, if specified\n",
    "    if adjust_params is not None:\n",
    "        best_params = adjust_params(best_params)\n",
    "\n",
    "    # Re-fit the model with the optimal hyperparamters\n",
    "    learner = LEARNER(**best_params)\n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(learner, '../saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0002751641657386846, 'l1_ratio': 0.5503451245739095, 'max_iter': 1000, 'random_state': 1, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 50\n",
    "N_FOLDS = 20\n",
    "\n",
    "# Search space (also includes constant parameters)\n",
    "space = {\n",
    "    'max_iter': 1000,\n",
    "    'tol':1E-3,\n",
    "    'random_state': 1,\n",
    "    'alpha': hp.lognormal('alpha', np.log(1E-4), 4),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters (defined above)\n",
    "find_best_hp(ElasticNet, space, model_name='el_up',\n",
    "              X_train=X_train_up, y_train=y_train,\n",
    "              max_evals=MAX_EVALS, n_jobs=3, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model, if necessary\n",
    "el_up = joblib.load('../saved_models/el_up.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEXCAYAAACzhgONAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3XmcXFWd8P/PvVXVVb2nO+nsGwnJIQtJSFhkFwiIgLiACzgqqIOOOvgbXB4f9RmXeXQWfZQZh3F4PW4oT1xQUEBAVg2CAklISAKcQEggezrdnaS32u/vj3tvdXV3bd3pW1236/t+vZTU2ud0Uvdb53zP+R7DsiyEEEKIfMzxboAQQojKJoFCCCFEQRIohBBCFCSBQgghREESKIQQQhQkgUIIIURBwfFugJiYlFLzgZ3A1qy7DeDftdY/GuF7zQEeBJLA32mt/zJW7fQzpdQZwEe01h8f5etPAr6ttb5mjNrzj8AWrfXvxuL9ROWQQCG81K+1XuXeUErNArYppTZorV8YwftcBBzUWq8d8xb62zJg9gm8fh6gxqgtABcDL47h+4kKYciGO+EFZ0SxTWvdMOT+Z4F/01r/Win1EeAT2FOgHcCntNYvK6V+ArQCC4EeYAbQDGzSWl+klLoJuBlIAYec1+0Y8rr7gWlAH3Cq8+d7nZ/zNmA68FGt9eNKqcXAbUCj87M2A+/VWkeVUlHgX4DLnMf+TWv9facv/xP4EPZI5xXgBq31sXz9yvE7KtSP40675wAvAB/UWvdkvXYO8JTze7lba32jUuptwJeBGqffn9Va/0UpdQrwQyCCPar7AXA7oIFZwHqt9VuGtO1dznulnfZ9Tmu9XinVDPy707YQ8BjwOeBjwL8C7cAtWut7hvZX+JfkKETZKKXOBk4GnlFKXYh9kT1fa30a8G9A9sWlTmu9TGt9FvCPwJNOkLgY+DxwkdZ6JbAO+K1Syhjyuv/h3F6N/U33AuAzQI/W+hzsi90XnOf8LXCH1vpNTvtOAq50HgsDR5zXXAt8VykVUUpdDdwAnK21Xg7sAj5VQr/c30WxfqwBLgeWAPOBd2e/Xmu9J+v3cqNSahHwTeAK5+feBNytlKrHvpDfp7VeA1zh/C4s4KPAzqFBwvEt4BNa69OB/wW82bn/u8BG571OA6ZgB4bbgA3YAUWCxAQjU0/CS7VKqc3On4PAEeD9Wus9Sqm/x74oP61UZvajRSnV6vz5z3ne83Lgl1rrdgCt9U+UUv+OfTHN9br7tNYJ4KBSqhd4yLl/J/boA+B/AJcqpT4PLAZmAtkjIXfOfRN24KgH1gJ3aa27nHbcAqCU+rd8/dJad46gHw9prWPOe27Nams+l2KPeB7L+rlppy33AD9VSp0JPArcrLVOZz0vl18A9yilfg88gh3wAK4CznRGTQC1RdolJgAJFMJLg3IUQwSAn7nf/JVSJvYFust5vKfA6+JD7jOwp0FyvS425HYix3v+HPuz8Cvg98Bc5z1d/QBaa8u5uBrY002ZeVul1CRgUgn9KrUf/Vn3W0Pak0sAeExr/d6sNs0B9muttzgjjkuBS4CvKKXWFHozrfWXlFI/cl5zA/Zo7Ezn57xba/1SVr9l/nqCk6knMV7+AFynlJrh3P449nx3MQ8B71NKtQEopW7EzgO8egJteQvwda31L53bZ2FfEAt5FHiXUqrJuf1V4BZK79dY9CPJQGB5DLjMyUeglLoCO7dRq5Rah51z+QV27uQ4dh4n+/UZSqmgUmo39jTefzuvWaGUCjv9+wellOHcvhf4VI72iAlEAoUYF1rrh7GTn48opV4ArgfepbUu+O1Ua/0I9jz540qp7dj5gKu01ukTaM4XsadZtmInef+EPWVTqB0PAD8GnnJeNx34Uqn9GqN+/BVYoJS6W2v9InZe4hdKqS3APwFXOwnwfwLe79z/DPZU1HrsFUpRpdSzWbkRtNZJ4P8D1imlNgF3AR92psJuxp5624odiLYyMC11L/DPSqkPjaAPwgdk1ZMQQoiCZEQhhBCiIAkUQgghCpJAIYQQoiBPl8cqpa7H3t0ZAm51NuVkP74aO3lYA+wB/kZrfdTLNgkhhBgZz5LZTl2fP2PvMI0BTwPXOasz3Oc8CXxTa/2gUur/YK+7/3IJbx8GzgAOYJcXEEIIUVwAe2PmcwzfY5SXlyOKtcDj7m5UpdSvsUsgfD3rOQHAXYdeB3RSmjOAJ8eonUIIUW3OJ3/1g2G8DBQzsb/xuw5g7+zMdgvwsFLqVqAXe6NTKQ4AdHX1kk6PfEQ0eXIDHR35Nv5ObNXad+l3dZF+52aaBi0t9TD42lyUl4HCZPDWfgO79gwASqla7IqWa7XWzyqlbgF+ykAxtkJSgNvhUZk8uaH4kyaoau279Lu6SL8LGtGUvZeBYi/28MY1HdifdXs5dk7iWef27dg7SEvW0dEzqhFFW1sj7e3dI37dRFCtfZd+Vxfpd26maYwqgHq5PPZR4BKlVJtSqg64hoHKnWDXtJmjBkpYvh07wSKEEKKCeBYotNb7gC8BT2AfBLPOmWJ6QCl1ulOe+QbgV05NnA8DN3rVHiGEEKPj6T4KrfU67ANZsu+7IuvPD2KfhSyEEKJCyc5sIYQQBUmgEEKUlWVZSNVqf6m6E+76ognu//NrnL5oMgFz7ONkV3eMp7YeIJZI8a4LFmAYxQ4mG51X9x3jhZ1HOGlGEwtnNdNUVzOq9+npT3DHQy+zcGYzl50xB9Ms3t4trx7h1X3HmDWlnjnTGpneWuvJ73IsxBIpNrx8mHj6AO2dvfRFk/TFkvRHE/TFkqTSFoZhYBpgGgaGYWAYUBcOMrWljmmttUxrqWNaSy2tTRFM0yAaT3Kos59DXX0c7OzjUGc/x/virF7cxjnLphOuKXbm0YlJptJ0dsc4crSf9qP99MdSnLdiBg21oz8zKJ22eHTjXt441M3bzpnPtNa6MWyx7WhPjNt/tx29Z3CVHsP5PwP7dw84/7Vvm4ZBTcgkHAoQrgnY/3X+Z1kWybRFKpUmlbZIpizSaYvZ0xs5ZXYzpy6cXPCzkbYsDnT0cbizD9M0CAZMggGDgPNfgI5jMdqP9tN+rJ/2Lvt3frQnzqVnzOGd559U9DOetizu/IPmyLEot7w334GPxR3s7GPDy4fZvquTq86dz7L5xU7HHTtVFyj2H+nj9nu2UnPNqZy2qG1M3jOZSrPl1SM8+cIBtr7Wgftl6epz5xMKenPReOS5PTz38uHM7WkttZw8q5mFs5o5ZV4L00v4oPf0J/j2z59nz+EeNup2NujD3HjFEmZNyb0/pas7xrpHdrBxR/ug+0NBk1lT6pk7rYFFsydxzvLpJxwg+2NJDAMiNaP7J9ofS/L4pr08/Nweuvvs009rgia1kSB14SB1kSD1tSGCpknaskhbFpblftuFjuMxXnqji3hi4ByhYMCgLhzkeN/g01Rbm8LUBAP87A+a3/xxJxesnMnFq2cxZdLYHSe9fVcnDz7zOoe7+uk8HiM95Bv5My8e4nPXraIuMvJgsbe9hx8/8DK7DhwnYBo88+IhLlkzm6vPnV/0/SzLKunv+tV9x7jtnq30x5K89ay5hIKm/fseeKfM58a+337Qwg5i8WSaWDxFLOH8L56ipz+BaRgEAgZB5yIfDhmYpsGLr3Xw1Jb9GMCCmU2sWDiZlSdPobm+htcOHOe1/fb/dh88Tn+stC0F9ZEgUybVMmdaI9NaU9z/9G5MA95x/oKCv5+fPqRZv2U/AdMgbVmYI/hs7DvSy8aXD7NBH2Zvey8AC2c20TzKL4ajVXWBYv6MRuprQ2zS7SccKDqPR3n4uT38ZftBuvsStDSGufLseUTjKR7dsJdE0iLk0W+4P5Zk7tQG3n/ZYl7dd4xX9x7jhdc6eGrbQQzgqnPmc/V58/N+0+/ui/PtX2zmQEcf//CelfREE6x75BW+9uNneft5J3H5WXMzr02nLR7ftJe7179GOm1x7ZsXcsma2bR39fPG4W72HO7hjUN2sFm/5QCL50yi7QQukpZl8Z1fbuZYb5x/vOGMEX1T7o0meHTDXh7dsIfeaJLlC1q56uz5nLliJke7+kbcjqM9cQ519nGoq49DXf309ieY2uKMMlrrmNpSm/lm++q+Yzy6wQ5Of3juDU5b1MYla2bTWBviaG+Mo91xjmX9d/6MJi49fXbBLxPptMW9T+3ivqd2M7k5wsmzmpmyrJa25ghtk2qZMinC3vZebrt7K9/51RY+895V1IZL+0eXTKX5/V9e5/6nd1MbDvKxq5dxytxJ3L3+NR55bg9PbzvIO84/iQtXzcz8W7Asi/1HenlhZwdbdnawc98xlp/UylXnzmfhzOacP2f9lv387A+a1qYwt7zndOZM9X4j3OTJDWzcvp8XXu1gy84j3PPkLu55clfm8YBpMLutgbOWTmfBjCZmtdVjWfbvJHt0YlkWrU0RpkyKUJ8VNNOWxY8feIl7n9pNTSjAFW+aN6wNlmWx7pFXWL9lPzMm13Ggo4+evgRN9cUv8r3RBP/6/55nb3sPBrBodjPXrV3EmsVttDZFxuR3NBJVFyiCAZMzl07j2e0HSabSBAOjmzJJWxa33rWFAx19rFo0hfNXzGT5Sa2YpsETm/YCkEim8OpX3B9PUl8bYtHsSSyaPQnOsv9hth/t576nd3Pf07vRb3Rx09XLhv3D6u6L862fb+ZgZx83X3MqyxdMBmDJvFbufFjzmz+9xkbdzoevXEI6bXHHQy+z60A3y05q5QNvUUx1gsDsqQ3MzvrQP7+jne/dvZW+aPKE+vbK3mPs3H8cgB/c/yI3X7ui6LewWML+hvfYxr1E4ylOWzSFq86Zz0kz7FJioxnZGYZBS2OYlsYwp8xrKfpc9++i83iUJ57fx58272fTkNEXQG04SENtkA26nfVb9nP92sWsWDh52POO98a5/d7tvPR6F+eeOp2/uUwRDg3vx5TmWj7xjuX812+3cetdW7jlPauKTn/tOnCcHz/wEnvbe3nT0mm8b+2izBTNjVcs4ZI1s/nFY69w58M7eHzTPi47Yw6vH+pm684OjhyLAjBnagPnnjqDjfow3/jpRpbNb+Gqc+aj5tq/q2QqzbpHX+GPz+9j2UmtfOzqZSc0PTYSpmkwf3oT86c3cfV5J3GsN87WnR30xZKcNKORedMaqcnxuyz5/Q2DG9+6hEQyza//uJNQwOTSM+ZkHrcsi1898SqPbdrLW86cw8mzmrntnm10dcdKChR7DvWwt72HK8+exyVrZjOpITzqto6FqgsUAGefOoMnNu5lx56jLB3lPN9G3c7e9l5uettS3rRs+qDHgkE7+CSSJ3KMc2HRWIrm+sH/eAzDYGpLHR+5cilL57fy0z9ovvKjZ/nwlUsyo6djPTG+9fPNHOrq4+ZrT2X5SQMXqOb6Gj75zlN57uXD/OwPmq/9+DksCxpq7W+bZy6ZWnCaIeJ8k+2PnVigeGTDHuojQa540zzu+uNOHvzr61x59vy8z48lUvz7XVvQbxzljCVTufLs+WX51ppPa1OEay5cyNvOmc+WnR1YlsWkhjCTGmpobghnLvbbd3Vy5yM7uPWuLZy2aArXXbIoM121Y89Rvv+7bfRFk9z41lM4f+XMgj/ztMVt3HT1Mv77d9v4j9+8wKevXZHzQnigo5dHN+zlj5v3MakhzM3XrGDVoinDnjd3WiOfu+40nn/lCL96/FV+8uDL1IRMls5r5Yqz57FiweTMF5D3XXIyf3x+Pw89+wb/uu55Fs9uZu3pc3h4wx5e3XuMt75pLtdcsLCk/JdXmutrOG/FjDF9T9M0+OhVS0mmLH7+2CuEQiZvXjULgHuefI0/PLuHS1bP5j0XncxrB+wvPl3dMeZNbyz63p3ddjA+99QZ4x4koEoDxWlqKjVBk4072kcVKNJpi9/9eRczJtdx5pJpwx4PuYEi5V2g6I8nqS3wrfHsZfaQ+r9/t53v/WYra9fM5vKz5vKfd2xwgsSKvMmwM06Zipo7ibueeJWaUIB3XbBg0LA7n7oxCBRHjvWzaUc7l585l8vPmsvrh7q5e/1rLJzZnPNbfTyR4nu/eQH9xlE+etVSzl4+Pce7jo+aUIAzTpma9/FlJ7Xy9Q+fycPPvcF9T+/mSz94hqvOnkcgYHL3n16jbVKEf3j3SuZOK35hAfvvLZlayg/ue5H/vHsrf3/NCsD+Zr/5lSM88fw+Xnq9i2DA4MJVs7j2woXURfJfAgzDYPXiNk5dMJm97T3MbqvPOTKL1AS5/Ky5XLx6Fuu37OfBZ97gv367jZqQycffviznZ2SiCAbsPv7n3Vv52UOaUMCk83iU+59+nQtWzuS6SxdhGAatjXZQ7eoprbJ3V7f9vJbG8Q8SUKWBIlITZPmCyTy/o533X7p4RMklgOdePsz+I7187OplOb8lhQL2h8nLEUV/LJX5Bp/PtNY6vviBNfz6jzt5ZMMennh+H4GAyaevXVE0QDbV1fCRK5eOqE2RsN3v/vjoA8Xjm/ZhYHDx6tkYhsGHLj+FNw718N/3buerN54x6NtVIpniP+/Zyku7u/jwlUsqKkiUKhQ0ufLs+Zy9bDq/eOyVzDz66aqNG69YUnK+wXX2sukkk2l+/ODLfP+321i6YDIPPL2Loz1xJjdFuObCBZy/YmZJ0x/ZbXSn8AqpCQVYe/ocLlw1i436MHOmNeZdGDGRBAMmn3zncm696wV+9PuXsIBzlk/ng5erzLWlqT6EYUCXM1IoprM7Rn0kmHOqcTxUZaAAWLO4jU072tm1/zgLZ+VOwuXiJhdnTqnP+20x5PHUk2VZRONJasPF/xGFgibXrV3Eknkt3P+X3Xz46uXMnORNMqy2xh1RjO4sqVg8xfrN+1m9eAqTm+021oaDfOKdy/nfd2zg9t9t57PXrSJgmiSSaW67ZxvbXuvkhreewrmnju20Qrm1NkX4xDtP5aXdnRztjfOmpdNGvXLs/JUzSabS/OzhHWx+9QjLF7TywbfMZsXCyWWZ/gkFzWHTsRNdKBjg5mtWcPu922mqr+GDb1GDvoAGTJPm+prMSKGYruMxWhrLn7TOp2oDxcqTJxMwDTbtaB9RoHj2pUMc6Ojj796xPO+HzutAEU+ksayBC3MpVi2awqpFUzytqul++42OckTx9PaD9MWSg5KCALPbGvjAWxQ//P1L/PbJXbz9vJP4/m+38cLODj54ueKCIvP3frJkjNbGX7R6NjOn1LNw3mSClncjWzEgXBPg5mtX5H28pTHC0RIDRWd3lNamyph2giremV0XCXHKvBY27mgveZeoPZrYzey2etao/Etrvc5RuFM7xaaeyi0UtDcp9Y0iR5G2LB7dsId50xs5OUfgPvfUGVywcia//8vr/Mv/28TmV4/wN5ctziQPxXBqbgszqmDqxy9aGsN0lhoojsdorZD8BFRxoAB7+ulwVz/7nI0sxTzz4iEOdvZx9bknFcxrhALejijcZHGhZPZ4idQEiY5i6unFXZ0c6Ojj0tNn551yef+li5g7tYHX9h/nurWLuHj17BNtrhBl09IY5mgJyexE0t5MWCmJbKjyQHHaoikYkHOt+1CpdJp7n9rF7LYGVhcYTYD3U0/RuH0hrrQRBUBtODCqZPYjG/bSVF/DGafkXyETCga45X2r+Px1p3Hp6XPyPk+IStTSGKY/liq6KtAddYzHxrp8qjpQNDeEWTi7eVhJilz+uv0Qh7r6eft5hUcT4H2gqOQRRW145COKAx29bH2tg4tOm5X53eXTVFdTdPObEJWoxVmxV2xU0XW8spbGQpUHCoDVi9rYc7iHw0f78z4nlU5z39O7mTu1gdWLh29OGsrzHIVzIR7p0slyqK0JjjhH8djGvQQDBm8+TfINYuJyL/zFVj51yYii8rjTSJt0/lHFX7Yd4rAzmihlyaL3U0+VmcwGd0RReqDoiyZ4autBzloyjeYRrO0Xwm9KDRTuruyWCtiR7ar6QDF1Ui1zpzaw6ZXcgeL1g938Zv1O5k1rzFnqIJeBZPbo9hMUU8lTT5ER5ijWb7FLsq+VnIOY4CaVHCiczXYV9Pmu+kABsHpxGzv3Hhs2d/j0tgN8886NdgGwK04peQOU17We+t1k9ihLcHupNhwc0Ya7P23ex+LZzSXVvxHCz8KhAPWRYNEyHpW22Q4kUAD29JMFPP/KEcCpevnIDn5w/0ssmNHEV244o+R6O2BXlgwGDM9yFNFYkmDAKJr4HQ+1NUH6Y8mS9qakLYvDXf2ZaqNCTHSTGsNFN91V2mY7qOKd2dlmTalnakstm3a0s3pxG9//7TZ27DnKpafP4d0XLRxVKfJQ0PR0RFGJowmwl8fatfzTRUt7R2MpLChYmE6IiaSlofimu67uWEm1tcpJPqHYVTLXLG7j4ef28PWfPEdvfyJn+fCRCAUDJL1KZsdKq/M0HmrDA/WeigWKvph9UlxdBSblhfBCS2OYPYd78j6eSKbo7ktU1K5skKmnjNWqjVTaImAafPEDa064qFko4OGIIpYcUZ2nchooDFg8oe0ecDSa4zuF8KOWxjDHe+Mk80xLD5QXr6wcRWVebcbBwpnN3PKelcyf0TQmp3CFgiZxL6eeKvRb+EhKjQ8EisrsixBjraUxjAUc64lnKiRn6zzu7qGQEUXFWr5g8pgd1ehljiIaK3xo0XgaSanxXidQ1EugEFUis5ciz8qnSjuwyCWBwiOhoDmiVU/JVLrkKrbReKoid2VDVqnxUqaeJEchqox78Fa+lU/uZrvWCpt6kkDhkZHkKOKJFP/wvT/z3MuHS3p+fzxZsVNPbpK9lDIe/TL1JKqMW5Yj38qnStxsBxIoPDOSqae+WJLeaJIDHX0lPb8/lqrYqadI5vCi0qaeDCqzFIkQXqiPBAkGzLwjCnuzXWVNO4EECs+MJFC4Se/e/kTR5yaSaZKpdMVeXEe06imWpDYcHPGZ5UL4lWEYtDTW5M1R2JvtKmvaCSRQeGYkOYp4wv727SZ3C3ELAlbqiMI+5c4scXlsQqadRNVpaYzQdTya87GubhlRVJVQwCRZYlFAd+TRGy0+onDrPFVqMhvcw4uK970vmpRAIapOS2M454jC3WwngaKKjGjqyRlR9JUyonC+qVdqCQ+wp59KWfXUG0vKiidRdVoawnR1x4etcsycQ1FhK55AAoVngiOYehrRiMItMV6hJTzArSBb2qqnetmVLapMS2OYZCpNz5CcZGYPRYVttgMJFJ4ZVTK7hBGFb6aeSk1my9STqDL5DjDK7MqWqafqEQqYJFMW6RI20cWdXEZvf6LopruBqafKHVFEaoIl5Sh6ownZlS2qjnuA0dDzbzIn20mgqB7uWRGlVJBNJOznpNIW8UTh51fyoUWuUkYUyVSaeCItOQpRddwRw9BNd53dMerCwYr8bEug8IhbYruUPEV28cBieYqoT3IUxTbcSeVYUa2a6mswGF7Go+t4rOKKAbokUHgkNILjUONZy2iHJriG6o/bu5nDocoOFMVOuXNLfMjyWFFtggGTpoaaYTkKew9F5a14Ao/LjCulrge+DISAW7XWtw15XAG3Ay3AQeB9WusuL9tULjUjCBSJrOmmYktk+2N2ifFSz+8eD5Ea+5S7RDJNTZ6AlhlRyNSTqEL2EtnhOYr5Myrz7HjPRhRKqVnAN4DzgFXATUqppVmPG8C9wL9orVcCzwNf8Ko95TayEcXIpp4qedoJsk65KzD91Of0U5bHimo0dNNdJW+2A2+nntYCj2utO7XWvcCvgWuzHl8N9GqtH3JufxO4jQkiFBjd1FOxJbLReKpiT7dzDRyHmr8v7tSTLI8V1ailMTwoR1Gp51C4vPyUzgQOZN0+AJyZdftk4KBS6ofAacBLwN972J6yyowoSkhmJ5JpwqEAsUSq6IjCLjFe4SOKEgoD9srUk6hiLY1heqNJYokU4VBgYFd2BRYEBG8DhQlkZzMNIPuqGQTeDFygtd6glPon4DvADaX+gMmTG0bduLY2b+cCpzibZ+rqw0V/lhkwaW4M03G0H0yz4POTaYvmhsgJtd/rvk93+h6urcn7swxnxDVvTkvZEvNe97tSSb8rz9yZzQCYoSBtbQ1se+MoAAvntp5wu73ot5eBYi9wftbt6cD+rNsHgVe01huc2z/Hnp4qWUdHD+l0aafCZWtra6S9vXvErxuJ3h5788yRjh7aJxX+lnC8J0bQNKiPBGnv6C3Ytu7eOE11NaNufzn6Hu+PA3DwcDcz8/S9vbOXYMDgWFdvWRLz5eh3JZJ+V6agsyJw5+udhLB4fZ8dKEgmT6jdxfptmsaovmB7maN4FLhEKdWmlKoDrgEeynr8aaBNKbXSuf02YKOH7SmrkeQoEsk0oaBJXSRUNEfRX8HnZbsipeQooknqIqGKXr0lhFcmDSnj0VXBm+3Aw0Chtd4HfAl4AtgMrNNaP6uUekApdbrWuh94J/B/lVLbgYuBz3jVnnIb0aqnRIqaoEl9bbCEHEXlnpftcgNZ0UBR4f0QwiuZek/OyqfO47GKLAbo8vSTqrVeB6wbct8VWX9+hsEJ7gljJIEikUwTqQkQCJgc64nnfV46bRGLpyq6zhOUvjxWNtuJahWpCVIbDg4aUVRieXGX7Mz2yEhLeISCAeojhUcUUR9UjgV752koaBY8k6IvJocWierW0jiw6a6zO1qxS2NBAoVnRraPIk1NqHiOInMMaoUHCrCnn2TqSYj83EDhbrar1DpPIIHCMyObekpR44wo+mPJvCu5+n1QYtwVCRcuNd4rhxaJKtfSEOZoT4wuZ7pZRhRVKBiwV/OUlsxOEwqZ1NfaF86+PN/E/XBokau2Jv8pd5Zl0S9TT6LKTWq0A0XH0X6gMo9AdUmg8IhhGPYpdyXlKJxVT86FM1+eIlNivEKX0GUrdCZFLJEilbZk6klUtdbGMJYFuw/a+x5k6qlKhQLFj0O1LItEwk1m2yOK3v7CIwo/TD3ZpcZzTz0NnEUhgUJUL3cvxc79xwGZeqpapZybnUxZWOCMKJyppzwjikyOosJrPYG9/M9Nvg81cBaF5ChE9WppcAPFsYrebAcSKDxVSqBIOJVj3Q13AD3Fpp58MGVTF86fo5ARhRBkNtit3+D1AAAZRElEQVQd64lX9GY7kEDhqVJyFO5ZFKFQIPMNeyJMPUXCAfpjqZyn3MmhRUJAY20os+ilkqedQAKFp0JBk2SREUU8kTWicL5h55t6isaT1IRMAmbl/7XVhoOkLWvQoUyu3syhRRIoRPUyDINJzvRTJa94AgkUnrKnnvLvJYCBEUVNKEAwYBKuCeTddNcfq/xDi1xuvadcu7MlRyGEzR1JtMqIonqVsurJfdzdoFeojEc0nsxUZq10heo99UfdXEvlT6EJ4SU3UEiOooqFgoHiOYqsqSewz5DOm6OIpSq+xLirUKnx3mjSLoLogyk0IbwkU0+CUNDMOUefzR1R1DhFBOsjwfzLY+NJX6x4gsKlxvtiUjlWCBiYcqr0ZLZ8Wj1UyvLY+LCppxAHO/tyPjcaS9I4qXZsG+mRzNRTjk13dkFAyU8IcfopU+mNJpk+uW68m1KQBAoPlZKjiLv7KEJ2oKgrkKPoj1X+oUWu2gJTT/bpdv7ohxBeam2K8M4LFox3M4qSqScPlTSiSAyZeqrNX2o8Gk/6Z9VTJpmde9WT7KEQwj8kUHiolA13mVVPoYFVT4lkOpPkdtkVV1O+KN8BA5sCcy6PjSZkD4UQPiKBwkMlbbhLDl/1BAwbVcSTadKW5Zupp2DApCZo5lwe2xdLUiuBQgjfkEDhoVDAJJW2SKXzB4tEjqknGF5qfKDEuD9GFOAcXjRkRJFO2yMjObRICP+QQOEhdyVTMpn7xDqwRwoB08A07ZovdZkyHoMvsJk6Tz4ZUUDu41Azu7J91A8hqp0ECg8F3eNQC+Qp4slUZsUTQEOmMODgEYWfjkF1RcJBokOmngbKd0igEMIvJFB4qJRzsxNJ+9AiV13mlLvB38T9dLqdK1epcXczoQQKIfxDAoWHQgE3UOQvDBhPpDKJbMhOZg8ZUfjovGxXJNfUk5QYF8J3JFB4qJQRRTyZpiY0MKKIhAMYRo4RRdw/p9u5ch2H6gYKSWYL4R8SKDzkrmQqlKOwp54G/hpMw7ALAw4dUTgXXD9NPdXmOA5VchRC+I8ECg+VNKIYMvUE9kV06Kon94Lrp9LctZHhp9z1ZUqMS6AQwi8kUHio1GT20EBhlxofPqIImAbBgH/+ymprnFPuEgP9740mMA3DV6u3hKh2/rnq+FCpOYrsVU8A9bXDCwO6JcYNwxj7hnokkqPeU1/MLgjop34IUe0kUHhoYNVTkamnUI4RRY7lsX77Fu5Ok2WvfJLKsUL4jwQKD4VK2nCXziS9XfWRYM6pJ7/N67uJ9+yVT/ZZFP7qhxDVTgKFh0recBcamswO0RdLks5KAtslxv02osg19SSn2wnhNxIoPBQsKUcxfNVTQySIZQ0u0W2XGPfXBTZXqXF76kn2UAjhJxIoPFQsR2FZFonE8GR2XY5S4346L9tVl+M4VJl6EsJ/JFB4aGDqKXcJj2TKwoLhy2Nr3XpPA3mKaMx/U0+RHMeh9kaTcmiRED4jgcJDAdPAMPInsxNDDi1y5Tq8qD/u36knN0eRSKZIptKSoxDCZyRQeMgwjILnZsfcQ4tCw1c9wUCp8WQqTSKZ9t2Iwj3lLupMPUlBQCH8ydNAoZS6Xin1olLqFaXUJws870ql1C4v2zJeQoH8gcIdUYSGTT0NHlFEfXhokas2HMzUd3L7I8lsIfzFs0ChlJoFfAM4D1gF3KSUWprjedOAbwMTcqtuoRFFPFl4ROGe3eDHQ4tc9uFFdvulIKAQ/uTliGIt8LjWulNr3Qv8Grg2x/N+AHzNw3aMq1DQLJCjSGeeM/g1AWqCJr39g0cUfqoc67KPQ3WnnuTQIiH8yMtAMRM4kHX7ADA7+wlKqZuBTcBfPWzHuAoFA/lHFIncyWywL6a9Q0YUflseC86ZFO6IQnIUQviSl59YE7CybhtA5oqplFoOXANcwpAAUqrJkxtG3bi2tsZRv3YkaiNBDNPM+fP2dPQDMHVK47DHmxvCJC27na8f6QNgxrThzxuNcvUdYFJThP3tPbS1NWKGjgAwZ9YkWhojZWuDq5z9riTS7+riRb+9DBR7gfOzbk8H9mfdfjcwA9gA1AAzlVJPaq2zX1NQR0cP6bRV/IlDtLU10t7ePeLXjYZhQW9fPOfPa+/oAaC3Jzrs8XDQpOtYP+3t3Rw8bD8WzfM+I1HOvgOYlkWP0+5DR+z+9vfESA6pjuu1cve7Uki/q0uxfpumMaov2F4GikeBryql2oBe7NHDTe6DWuuvAF8BUErNB/44kiDhF4VyFJmpp9Dwqaf62hDtR6PAwD4EP049RbKOQ+2PJqkJmsNyMkKIyubZJ1ZrvQ/4EvAEsBlYp7V+Vin1gFLqdK9+bqUpadVTcPhqprpIkL6Y/a3b3Yfgx1VPbo7Csix6o1IQUAg/8vRTq7VeB6wbct8VOZ63G5jvZVvGSyhoksy7j8JZ9ZRrRBEJZVY99ceSGEDYl4EigGVBLJFyDi2SPRRC+E1JIwql1JuVUh9y9jxk3/8hb5o1cRQeUeRf9VRfGyKWsEte9MeTRMIBTB+eCpd9JoUUBBTCn4oGCqXULcDtwHuAl5RSF2U9/GmvGjZRhAIF9lEk8k89Zcp4RJNEYykiPtxDARBxTrmLxpNyup0QPlXKiOLDwBla6yuB64FfKqVOdR7z31fcMiuWowiYBqY5/NeYKQzYn/BliXGXO4LoiyXl0CIhfKqUQJHQWh8H0Fo/BHwWuFcpNZnB+yREDsWmnnKteILsMh5JX5YYd7kjoagz9VQflhyFEH5TSqBoV0rdqJSKAGitfwrcDTwANHvZuImgYKBIDD8v2+UWBuyJJnxZYtxVO2hEkaRWRhRC+E4pgeLvsKef3uveobX+DLAemOdRuyaMUMAkbVkkc+QpEslU3j0FdVmFAft9PKJw2320O4ZlSfkOIfyoaKDQWu/UWp+vtb5jyP2fwwkUSqnPeNQ+33OPOc01qogn08Mqx7oGchRJon4eUTgBr+O4vXlQTrcTwn9OaMOds6kO4P1j0JYJKXMcas4RRTr/iCI8cByqPaLw5wXW3STYccwOFJLMFsJ/xmpntqx+ysMNBLk23cUTqZx7KMCuyVIXDtLbnyQWT/lyVzZAwDSpCZkcOe4GCklmC+E3YxUoZPVTHqGAM6LIESgSyXTeQAFQXxukszuKhT/rPLlqw0E63UDh434IUa2kOpvHMlNPOQJFLJHO5DByqYuEOOJM2bgb1/yotiZId59dt0pyFEL4jwQKjwUL5ijy76MAaIgEM4HCrzkKsOs9uSRHIYT/SI7CY4VGFPFk/n0UYI8oBk638++Iwt10Z4BvV28JUc1OKFAopS51/vitMWjLhFQsR5GrcqzL3XQH+LbWEwzkJWrDQV8WNhSi2hW9+iil1gDfAzqAG7XWR5RSc4FbgbcCtU45cZGDO7WUe0SRf9UTDJ7P93My282vyLSTEP5Uyojiv4DfAK8BX1ZKvR14AagHVnrYtgkhM6IYkqOwLItEkWR2fdZSUr/uzIaB/IoECiH8qZRPbrPW+v8opQLADuxy4x/XWv/C26ZNDAM5itSg+5MpC4vcZ1G4skcUfp7bd9suS2OF8KdSRhR9AFrrFBABrpQgUbp8JTwKHVrkyt6c5tcNdzAQIGSznRD+VEqgyM4+HtFaP+9VYyaifKue4u6hRXlqPQE01NoX2JqgSTDg35XMkqMQwt9K+eSaSqkWnICR/WcArXWnR22bEPLlKNypqHy1nmDgG7ifp50gK0fh834IUa1K+eSeChxhIDh0ZD1mAf6dEymDYND+tQ2feio+onBzFH5OZMPAii3ZlS2EPxX95Gqt/TvnUQECpknANIYFCvd2oRFF/UQZUWSmniRHIYQfSRAog2COU+7iieLJ7JqQSTBg+H5E4QaI+lp/BzwhqpUEijIIBYYHCvd2oRIehmFQFwn5erMdwMzJdXzkyiWsXtQ23k0RQoyCv69APpHr3OxYovjUE8CSeS3MmdrgWdvKwTAMzj11xng3QwgxShIoyiAUNPOueipUPRbgY1cv86xdQghRCpl6KoNcI4p4CVNPQghRCSRQlEGhHEWh6rFCCFEJ5CpVBvaIYnCtp1JKeAghRCWQq1QZ5MxRJGTqSQjhDxIoyiDX1FMsmSJgGpimHOQjhKhsEijKIBQKDM9RJNIFy3cIIUSlkEBRBrlGFPZ52fLrF0JUPrlSlUG+fRTFNtsJIUQlkCtVGYSCJslcIwqZehJC+IAEijLIteEukUzLiEII4QtypSoDN0dhWVbmvngiJTkKIYQvyJWqDEJBEwtIpbMChSSzhRA+4WlRQKXU9cCXgRBwq9b6tiGPvx34GvbpebuAG7XWXV62aTxkn5vtnn0dT6Rprq8Zz2YJIURJPPtKq5SaBXwDOA9YBdyklFqa9XgT8H3gSq31SuAF4KtetWc8ZQcKl6x6EkL4hZdXqrXA41rrTq11L/Br4Nqsx0PAJ7XW+5zbLwBzPWzPuAkFhgcKe+pJVj0JISqfl1NPM4EDWbcPAGe6N7TWHcA9AEqpWuALwPdG8gMmTx79gT5tbY2jfu1ItbbWA9DYXEtbm93mZMqiqTFc1na4xuNnVgLpd3WRfo8dLwOFCVhZtw0gPfRJSqlm7ICxRWt9x0h+QEdHD+m0VfyJQ7S1NdLe3j3i141WtC8GwMFDx6lxfiWxRJJUMlXWdkD5+14ppN/VRfqdm2kao/qC7eXU014g+/zL6cD+7CcopWYAT2JPO33Uw7aMq0yOwtmdbVkWiUSakEw9CSF8wMsRxaPAV5VSbUAvcA1wk/ugUioA3Af8Smv9vz1sx7hzcxTu7uxkKo2FnEUhhPAHzwKF1nqfUupLwBNADfADrfWzSqkHgH8E5gCrgaBSyk1yb9BaT7iRhTtycJPZmWNQpYSHEMIHPN1HobVeB6wbct8Vzh83UCUb/oYuj41nDi2qiu4LIXxOrlRlEBySo3CPRZV9FEIIP5ArVRnUDB1RyNSTEMJHJFCUwdCpJ/e/MqIQQviBXKnKYHiOwp56khyFEMIP5EpVBpkSHqkhU0+yj0II4QMSKMogmG/VU0h+/UKIyidXqjIwDYNgwMjKUciqJyGEf8iVqkyyj0OVqSchhJ9IoCiTUMDM2kfhrHqSqSchhA/IlapM7BGFPeUUT8qqJyGEf8iVqkyCwUCOEh4y9SSEqHwSKMokFMjOUaQImAamaYxzq4QQojgJFGUSCmblKBJpKd8hhPANCRRlEgqaJBIDq54kPyGE8Au5WpXJoBFFMiV7KIQQviFXqzIZnKOQqSchhH9IoCiT7A13iWRaRhRCCN+Qq1WZDNqZnUhJjkII4RtytSqTmqwchSSzhRB+IlerMgkOGlFIjkII4R8SKMpkcI5CVj0JIfxDrlZlEgqYJFNpLMtypp5kRCGE8AcJFGXijiCSqbS96kkqxwohfEKuVmUSckYQiWSaeFJWPQkh/EOuVmXijijiyTTxRDoTOIQQotJJoCiTUMD+VffHkoCcRSGE8A+5WpWJO6LojTqBQpbHCiF8QgJFmbiBoi+aAGREIYTwD7lalcnQEYXsoxBC+IVcrcrEzVH0ydSTEMJnJFCUSWZE0Z8YdFsIISqdXK3KZFgyWwKFEMIn5GpVJsOS2TL1JITwCQkUZeLmKGREIYTwG7lalcnA1JPkKIQQ/iJXqzIZmHpyRxQy9SSE8AcJFGXi1nbKjCikeqwQwieCXr65Uup64MtACLhVa33bkMdXAT8AmoD1wMe11kkv2zReggEDkByFEMJ/PLtaKaVmAd8AzgNWATcppZYOedqdwKe01osBA/hbr9oz3gzDGHTKnUw9CSH8wsuvtWuBx7XWnVrrXuDXwLXug0qpeUCt1vqvzl0/Ad7tYXvGnbvyKRgwME1jnFsjhBCl8TJQzAQOZN0+AMweweMTjpvQlrMohBB+4mWOwgSsrNsGkB7B40VNntww6sa1tTWO+rWjFQ4HoTdOpCYwLj/fNZ4/ezxJv6uL9HvseBko9gLnZ92eDuwf8viMAo8X1dHRQzptFX/iEG1tjbS3d4/4dSfKyWcTMI1x+fkwfn0fb9Lv6iL9zs00jVF9wfZy6ulR4BKlVJtSqg64BnjIfVBr/ToQVUqd69z1AeBBD9sz7twchZTvEEL4iWeBQmu9D/gS8ASwGVintX5WKfWAUup052nvB76rlHoZaAD+w6v2VIKBHIUsjRVC+Ien+yi01uuAdUPuuyLrz1uAM71sQyVxA4TsoRBC+IlcscooGJSpJyGE/0igKKNMjkJGFEIIH5ErVhlJjkII4UdyxSqjgRyFTD0JIfxDAkUZuTuypXKsEMJP5IpVRpKjEEL4kVyxykhqPQkh/EgCRRm5I4mwTD0JIXxErlhlJCMKIYQfSaAoo6DszBZC+JBcscpI9lEIIfxIrlhlJNVjhRB+JIGijGREIYTwI7lilZFUjxVC+JFcscpoclMEw4DWpsh4N0UIIUrm6XkUYrC50xr53qcvoC4iv3YhhH/IiKLMJEgIIfxGAoUQQoiCJFAIIYQoSAKFEEKIgiRQCCGEKEgChRBCiIIkUAghhCjIr2s1AwCmaYz6DU7ktX5XrX2XflcX6XfBx0ZUcM6wLOsEmjRuzgOeHO9GCCGET50P/LnUJ/s1UISBM4ADQGqc2yKEEH4RAGYAzwGxUl/k10AhhBCiTCSZLYQQoiAJFEIIIQqSQCGEEKIgCRRCCCEKkkAhhBCiIAkUQgghCpJAIYQQoiC/lvAYNaXU9cCXgRBwq9b6tnFukqeUUk3A08BVWuvdSqm1wHeAWuCXWusvj2sDPaCU+grwHufm77XWn6+Sfn8duBawgB9qrb9TDf12KaW+DUzRWt+glFoF/ABoAtYDH9daJ8e1gWNMKfUEMBVIOHd9DFiIB9e3qhpRKKVmAd/ALgGyCrhJKbV0fFvlHaXUWdjb9Bc7t2uBHwFvB5YAZyil3jp+LRx7zoXxMuA07L/jNUqp65j4/b4QuBhYAZwO/L1SaiUTvN8updQlwIey7roT+JTWejFgAH87Lg3ziFLKwP5cr9Rar9JarwL24tH1raoCBbAWeFxr3am17gV+jf0NbKL6W+CTwH7n9pnAK1rrXc63qzuBd49X4zxyAPiM1jqutU4AL2F/oCZ0v7XWfwIucvo3FXu2YBITvN8ASqlW7AvkN53b84BarfVfnaf8hInXb+X892Gl1Bal1Kfw8PpWbYFiJvaFxHUAmD1ObfGc1vqjWuvs4okTvv9a6+3uBUIptQh7CirNBO83gNY6oZT6GvAi8BhV8PftuB34EtDl3K6Gfrdg/x2/E7gE+DgwF4/6XW2BwsSev3UZ2BeRalE1/VdKLQMeAT4HvEaV9Ftr/RWgDZiDPZKa0P1WSn0U2KO1fizr7gn/71xr/Ret9Qe11se01keAHwJfx6N+V1ug2ItdOdE1nYFpmWpQFf1XSp2L/W3rC1rrO6iCfiulTnESuGit+4C7gTczwfsNvBe4TCm1GftCeTXwUSZ4v5VS5zl5GZcB7MajflfbqqdHga8qpdqAXuAa4KbxbVJZPQMopdTJwC7geuxk54ShlJoD/BZ4r9b6cefuCd9vYAHwNaXUedjfKt+OPSXzrYncb631pe6flVI3AG/WWt+olNqmlDpXa/0U8AHgwfFqo0cmAV9XSp2DvcLpQ8DfAHd6cX2rqhGF1nof9lzmE8BmYJ3W+tnxbVX5aK2jwA3Ab7DnsV/GTnhNJJ8FIsB3lFKbnW+aNzDB+621fgD4PfA8sBF4Wmv9CyZ4vwt4P/BdpdTLQAPwH+PcnjGltb6fwX/fP3KCoifXNzmPQgghREFVNaIQQggxchIohBBCFCSBQgghREESKIQQQhQkgUIIIURB1baPQohhlFK7sWviXAFs0Vr/bgzf+2Hgeq31EaXUA8BntdYvjtX7C1EOEiiEGHAx9n6DsZTZEKa1vmKM31uIspBAIYTtSuzy3N9SSqWwNzP9K3AhEMDe2HSz1vq4MwJ5Bruk9xexzwP4IlCDXbn1Dq31/1JK/dh57yeUUlcATwLXaq03KKVuAm4GUsAh7JLYO5RSPwGOA6di12t6Afig1rrH4/4LkZfkKISw/R7YAHxOa30P8AUgCazRWq/ErpnzL1nP36a1XoJdLuQzwIe01qcDbwL+p1Jqitb6Rue5F2mt97gvVEpdDHzeuX8lsA74rXPGAMAa4HLsMyTmM/FKZAufkUAhRG5XYddLet4pA/IOIPsQmCcBtNYW8DbsA5K+gn2anAHUF3jvy7FPm2t33uMnwCzsoADwkNY65pynsRVoHaM+CTEqMvUkRG4B4NNa6wcBlFIN2DWkXD3O/fXY01L3YAePH2EHFYP8AkB8yH0GdnE3gP6s+60i7yWE52REIcSAJAMX6z8An1JK1SilTOD/Av+c4zWLsM9l/rLW+j7s0t5h7GAAdg4iNOQ1DwHvc6p8opS6EegAXh27rggxdiRQCDHgXuCflVIfAv4Ju77/89groQzsXMRQLwD3Ay8rpV7CnoZ6ETjZefwu4E9KqeXuC7TWjwDfBR5XSm3HLhF9ldZ6Qh2uIyYOqR4rhBCiIBlRCCGEKEgChRBCiIIkUAghhChIAoUQQoiCJFAIIYQoSAKFEEKIgiRQCCGEKEgChRBCiIL+fweARM0aboJaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load full results from progress file\n",
    "el_up_results = pd.read_csv('../progress_el_up.csv')\n",
    "\n",
    "# Get validation score\n",
    "# Extract AP for each iteration\n",
    "r2_el_up = - el_up_results.loss\n",
    "\n",
    "# Plot r2 per iteration\n",
    "r2_el_up.plot()\n",
    "plt.title('Performance on test set')\n",
    "plt.ylabel('R_2')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals that optimizing hyperparameters to regularize a linear model is relatively simple, so performance seems pretty stable without much improvement for further iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8455297953337058"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_el_up = el_up.predict(X_train_up)\n",
    "r2_el_up_train = r2_score(y_train, y_el_up)\n",
    "r2_el_up_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475558636319633"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_el_up = el_up.predict(X_test_up)\n",
    "r2_el_up = r2_score(y_test, y_el_up)\n",
    "r2_el_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8455297953337058, 0.8475558636319633]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save performance\n",
    "r2_up_r = [r2_el_up_train, r2_el_up]\n",
    "r2_up_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the explained variance reveals that regularization gave us a boost in performance.  However, as mentioned above, we will save a comparison for a final notebook.\n",
    "\n",
    "Let's print the coefficients, ignoring the intercepts for all categorical variables in order to limit the size of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row0\" class=\"row_heading level0 row0\" >allsuites</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row0_col0\" class=\"data row0 col0\" >0.03674</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row1\" class=\"row_heading level0 row1\" >boutique</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row1_col0\" class=\"data row1 col0\" >0.03764</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row2\" class=\"row_heading level0 row2\" >casino</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row2_col0\" class=\"data row2 col0\" >0.65405</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row3\" class=\"row_heading level0 row3\" >cbd</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row3_col0\" class=\"data row3 col0\" >0.21244</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row4\" class=\"row_heading level0 row4\" >conference</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row4_col0\" class=\"data row4 col0\" >0.16512</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row5\" class=\"row_heading level0 row5\" >convention</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row5_col0\" class=\"data row5 col0\" >0.32810</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row6\" class=\"row_heading level0 row6\" >golf</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row6_col0\" class=\"data row6 col0\" >0.12423</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row7\" class=\"row_heading level0 row7\" >indoorcorridors</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row7_col0\" class=\"data row7 col0\" >0.01002</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row8\" class=\"row_heading level0 row8\" >log1_age</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row8_col0\" class=\"data row8 col0\" >-0.11188</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row9\" class=\"row_heading level0 row9\" >log1_landsf</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row9_col0\" class=\"data row9 col0\" >-0.00451</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row10\" class=\"row_heading level0 row10\" >log1_largestmeetingspace</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row10_col0\" class=\"data row10 col0\" >0.00020</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row11\" class=\"row_heading level0 row11\" >log_floors</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row11_col0\" class=\"data row11 col0\" >0.16970</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row12\" class=\"row_heading level0 row12\" >log_rooms</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row12_col0\" class=\"data row12 col0\" >0.44557</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row13\" class=\"row_heading level0 row13\" >log_sizesf</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row13_col0\" class=\"data row13 col0\" >0.14693</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row14\" class=\"row_heading level0 row14\" >multiproperty</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row14_col0\" class=\"data row14 col0\" >-0.02668</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row15\" class=\"row_heading level0 row15\" >portfolio</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row15_col0\" class=\"data row15 col0\" >0.20422</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row16\" class=\"row_heading level0 row16\" >restaurant</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row16_col0\" class=\"data row16 col0\" >0.01400</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row17\" class=\"row_heading level0 row17\" >selfrun</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row17_col0\" class=\"data row17 col0\" >-0.06929</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row18\" class=\"row_heading level0 row18\" >ski</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row18_col0\" class=\"data row18 col0\" >0.08424</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969level0_row19\" class=\"row_heading level0 row19\" >spa</th> \n",
       "        <td id=\"T_9ae70902_3e02_11e9_ba71_28d24441a969row19_col0\" class=\"data row19 col0\" >0.18737</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x296928f5160>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    el_up.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled\n",
    "Finally, we will re-estimate the model for the pooled case. Again, we do not drop the first category when performing one-hot encoding, since regularization effectively deals with collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables to include\n",
    "cats = ['location', 'category', 'operation', \n",
    "        'year', 'quarter']\n",
    "# Categorical variables to exclude\n",
    "cats_out = ['msa', 'tract', 'saleaffiliation']\n",
    "# Perform one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "X_train_cats = ohe.fit_transform(\n",
    "    data_train.loc[:, cats])        \n",
    "X_test_cats = ohe.transform(\n",
    "    data_test.loc[:, cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables excluded:  ['year_2', 'year_3', 'age_2', 'age_3', 'floors_2', 'landsf', 'age', 'largestmeetingspace', 'sizesf', 'rooms', 'floors', 'saleprice', 'log_saleprice', 'id', 'new']\n"
     ]
    }
   ],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "    data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats  + cats_out+ v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + cats_out+ v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train_p = np.concatenate([X_train_rest, X_train_cats], \n",
    "                         axis=1)\n",
    "X_test_p = np.concatenate([X_test_rest, X_test_cats], \n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + list(ohe.get_feature_names())\n",
    "\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 3.7701515352783604e-05, 'l1_ratio': 0.9952987471092946, 'max_iter': 1000, 'random_state': 1, 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "MAX_EVALS = 50\n",
    "N_FOLDS = 20\n",
    "\n",
    "# Search space (also includes constant parameters)\n",
    "space = {\n",
    "    'max_iter': 1000,\n",
    "    'tol':1E-3,\n",
    "    'random_state': 1,\n",
    "    'alpha': hp.lognormal('alpha', np.log(1E-4), 4),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters (defined above)\n",
    "find_best_hp(ElasticNet, space, model_name='el_p',\n",
    "              X_train=X_train_p, y_train=y_train,\n",
    "              max_evals=MAX_EVALS, n_jobs=3, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model, if necessary\n",
    "el_p = joblib.load('../saved_models/el_p.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load full results from progress file\n",
    "el_results = pd.read_csv('../progress_el_p.csv')\n",
    "\n",
    "# Get validation score\n",
    "# Extract AP for each iteration\n",
    "r2_el_p = - el_results.loss\n",
    "\n",
    "# Plot r2 per iteration\n",
    "r2_el_p.plot()\n",
    "plt.title('Performance on test set')\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining how the performance change over the iterations of the hyperparameters optimization, we see that it stayed almost stable.  This is due to the fact that hyperparameters optimization for this case is easy (compared, e.g., what we saw for gradient boosting, where we optimized over a higher-dimensional space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7225784583731365"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_el_p = el_p.predict(X_train_p)\n",
    "r2_el_p_train = r2_score(y_train, y_el_p)\n",
    "r2_el_p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7809748676982142"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_el_p = el_p.predict(X_test_p)\n",
    "r2_el_p = r2_score(y_test, y_el_p)\n",
    "r2_el_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7225784583731365, 0.7809748676982142]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save performance\n",
    "r2_p_r = [r2_el_p_train, r2_el_p]\n",
    "r2_p_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style>  \n",
       "<table id=\"T_bd378ea8_4149_11e9_b099_28d24441a969\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >0</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row0\" class=\"row_heading level0 row0\" >allsuites</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row0_col0\" class=\"data row0 col0\" >0.02801</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row1\" class=\"row_heading level0 row1\" >boutique</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row1_col0\" class=\"data row1 col0\" >0.12914</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row2\" class=\"row_heading level0 row2\" >casino</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row2_col0\" class=\"data row2 col0\" >0.68205</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row3\" class=\"row_heading level0 row3\" >cbd</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row3_col0\" class=\"data row3 col0\" >-0.04083</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row4\" class=\"row_heading level0 row4\" >conference</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row4_col0\" class=\"data row4 col0\" >0.22401</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row5\" class=\"row_heading level0 row5\" >convention</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row5_col0\" class=\"data row5 col0\" >0.28051</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row6\" class=\"row_heading level0 row6\" >golf</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row6_col0\" class=\"data row6 col0\" >0.13117</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row7\" class=\"row_heading level0 row7\" >indoorcorridors</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row7_col0\" class=\"data row7 col0\" >-0.01987</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row8\" class=\"row_heading level0 row8\" >log1_age</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row8_col0\" class=\"data row8 col0\" >-0.12246</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row9\" class=\"row_heading level0 row9\" >log1_landsf</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row9_col0\" class=\"data row9 col0\" >-0.02854</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row10\" class=\"row_heading level0 row10\" >log1_largestmeetingspace</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row10_col0\" class=\"data row10 col0\" >-0.01309</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row11\" class=\"row_heading level0 row11\" >log_floors</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row11_col0\" class=\"data row11 col0\" >0.29065</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row12\" class=\"row_heading level0 row12\" >log_rooms</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row12_col0\" class=\"data row12 col0\" >0.46736</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row13\" class=\"row_heading level0 row13\" >log_sizesf</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row13_col0\" class=\"data row13 col0\" >0.08141</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row14\" class=\"row_heading level0 row14\" >multiproperty</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row14_col0\" class=\"data row14 col0\" >0.02552</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row15\" class=\"row_heading level0 row15\" >portfolio</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row15_col0\" class=\"data row15 col0\" >0.25386</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row16\" class=\"row_heading level0 row16\" >restaurant</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row16_col0\" class=\"data row16 col0\" >0.04196</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row17\" class=\"row_heading level0 row17\" >selfrun</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row17_col0\" class=\"data row17 col0\" >-0.09624</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row18\" class=\"row_heading level0 row18\" >ski</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row18_col0\" class=\"data row18 col0\" >-0.07958</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_bd378ea8_4149_11e9_b099_28d24441a969level0_row19\" class=\"row_heading level0 row19\" >spa</th> \n",
       "        <td id=\"T_bd378ea8_4149_11e9_b099_28d24441a969row19_col0\" class=\"data row19 col0\" >0.27035</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x29692b800f0>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    el_p.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save performance\n",
    "Finally, we concatenate the performance of all linear models into a data frame and save it to disk. We will load this in the final notebook, where we compare the performance of these models with each other and with the performance of gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved_models/r2_lin.joblib']"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all R_2 scores from OLS\n",
    "r2_lin = pd.DataFrame([r2_up, r2_p, r2_up_r, r2_p_r],\n",
    "             columns=['Train', 'Test'],\n",
    "             index=['OLS, unpooled, no reg.',\n",
    "                    'OLS, pooled, no reg.',\n",
    "                    'linear regression, unpooled, elastic net reg.',\n",
    "                    'linear regression, pooled, elastic net reg.']\n",
    ")\n",
    "joblib.dump(r2_lin, '../saved_models/r2_lin.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('jupyter nbconvert --to html 2_modeling_linear.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\t\\\\Desktop\\\\projects\\\\hotels\\\\FINAL'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.764px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
