{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#OLS\" data-toc-modified-id=\"OLS-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>OLS</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unpooled\" data-toc-modified-id=\"Unpooled-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Unpooled</a></span><ul class=\"toc-item\"><li><span><a href=\"#All-logs\" data-toc-modified-id=\"All-logs-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>All logs</a></span></li><li><span><a href=\"#Logs-or-squares\" data-toc-modified-id=\"Logs-or-squares-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Logs or squares</a></span></li></ul></li><li><span><a href=\"#Pooled\" data-toc-modified-id=\"Pooled-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Pooled</a></span></li></ul></li><li><span><a href=\"#Linear-model-with-elastic-net-regularization\" data-toc-modified-id=\"Linear-model-with-elastic-net-regularization-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Linear model with elastic net regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Unpooled\" data-toc-modified-id=\"Unpooled-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Unpooled</a></span></li><li><span><a href=\"#Pooled\" data-toc-modified-id=\"Pooled-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Pooled</a></span></li></ul></li><li><span><a href=\"#Save-performance\" data-toc-modified-id=\"Save-performance-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Save performance</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pdb \n",
    "import glob\n",
    "import copy\n",
    "import math\n",
    "# import pickle\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    " \n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, \\\n",
    "    StratifiedShuffleSplit, cross_val_score, KFold\n",
    "#     GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression, \\\n",
    "    ElasticNet\n",
    "#     LogisticRegressionCV, SGDClassifier\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.ensemble import RandomForestClassifier, \\\n",
    "#     ExtraTreesClassifier\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.utils import resample\n",
    "# from sklearn.utils.fixes import signature\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval, pyll\n",
    "# import xgboost as xgb\n",
    "# from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# Set up pandas table display\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "# Set plotting options\n",
    "sns.set() # Use seaborn defaults for plotting\n",
    "%matplotlib inline \n",
    "\n",
    "# Load line profiler\n",
    "# %load_ext line_profiler\n",
    "\n",
    "# Adjust number of CPU cores to use\n",
    "N_JOBS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous notebook\n",
    "all_data = pd.read_csv('../../data_raw/all_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take another look at the data to make sure everything is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>allsuites</th>\n",
       "      <th>boutique</th>\n",
       "      <th>casino</th>\n",
       "      <th>cbd</th>\n",
       "      <th>category</th>\n",
       "      <th>conference</th>\n",
       "      <th>convention</th>\n",
       "      <th>floors</th>\n",
       "      <th>golf</th>\n",
       "      <th>indoorcorridors</th>\n",
       "      <th>landsf</th>\n",
       "      <th>largestmeetingspace</th>\n",
       "      <th>location</th>\n",
       "      <th>msa</th>\n",
       "      <th>multiproperty</th>\n",
       "      <th>new</th>\n",
       "      <th>operation</th>\n",
       "      <th>portfolio</th>\n",
       "      <th>quarter</th>\n",
       "      <th>resort</th>\n",
       "      <th>restaurant</th>\n",
       "      <th>rooms</th>\n",
       "      <th>saleaffiliation</th>\n",
       "      <th>saleprice</th>\n",
       "      <th>sizesf</th>\n",
       "      <th>ski</th>\n",
       "      <th>spa</th>\n",
       "      <th>id</th>\n",
       "      <th>tract</th>\n",
       "      <th>year</th>\n",
       "      <th>log1_landsf</th>\n",
       "      <th>log1_age</th>\n",
       "      <th>log1_largestmeetingspace</th>\n",
       "      <th>log_saleprice</th>\n",
       "      <th>log_sizesf</th>\n",
       "      <th>log_rooms</th>\n",
       "      <th>log_floors</th>\n",
       "      <th>year_2</th>\n",
       "      <th>year_3</th>\n",
       "      <th>age_2</th>\n",
       "      <th>age_3</th>\n",
       "      <th>floors_2</th>\n",
       "      <th>land</th>\n",
       "      <th>selfrun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130301.0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Small Metro/Town</td>\n",
       "      <td>Dothan, AL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>42446.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>Dothan/Enterprise, AL</td>\n",
       "      <td>2009</td>\n",
       "      <td>11.777610</td>\n",
       "      <td>3.891820</td>\n",
       "      <td>7.601402</td>\n",
       "      <td>13.815511</td>\n",
       "      <td>10.655988</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>4036081</td>\n",
       "      <td>8108486729</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>110592.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>361548.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Small Metro/Town</td>\n",
       "      <td>Florence-Muscle Shoals, AL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Independent</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>151421.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>Alabama North Area</td>\n",
       "      <td>2010</td>\n",
       "      <td>12.798153</td>\n",
       "      <td>3.401197</td>\n",
       "      <td>8.006701</td>\n",
       "      <td>13.815511</td>\n",
       "      <td>11.927819</td>\n",
       "      <td>5.308268</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4040100</td>\n",
       "      <td>8120601000</td>\n",
       "      <td>841.0</td>\n",
       "      <td>24389.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Upscale Class</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>357627.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>Suburban</td>\n",
       "      <td>Phoenix-Mesa-Scottsdale, AZ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Franchise</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>Crowne Plaza</td>\n",
       "      <td>24000000.0</td>\n",
       "      <td>142978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39</td>\n",
       "      <td>Black Canyon Corridor, AZ</td>\n",
       "      <td>2007</td>\n",
       "      <td>12.787249</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>8.594339</td>\n",
       "      <td>16.993564</td>\n",
       "      <td>11.870446</td>\n",
       "      <td>5.513429</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>4028049</td>\n",
       "      <td>8084294343</td>\n",
       "      <td>676.0</td>\n",
       "      <td>17576.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   age  allsuites  boutique  casino  cbd       category  conference  convention  floors  golf  \\\n",
       "0           0  48.0        0.0       0.0     0.0  0.0  Economy Class         0.0         0.0     2.0   0.0   \n",
       "1           1  29.0        0.0       0.0     0.0  0.0  Economy Class         0.0         0.0     3.0   0.0   \n",
       "2           2  26.0        0.0       0.0     0.0  0.0  Upscale Class         0.0         0.0     4.0   0.0   \n",
       "\n",
       "   indoorcorridors    landsf  largestmeetingspace          location                          msa  multiproperty  new  \\\n",
       "0              0.0  130301.0               2000.0  Small Metro/Town                   Dothan, AL              0    0   \n",
       "1              1.0  361548.0               3000.0  Small Metro/Town   Florence-Muscle Shoals, AL              0    0   \n",
       "2              1.0  357627.0               5400.0          Suburban  Phoenix-Mesa-Scottsdale, AZ              0    0   \n",
       "\n",
       "     operation  portfolio  quarter  resort  restaurant  rooms saleaffiliation   saleprice    sizesf  ski  spa  id  \\\n",
       "0  Independent          0        2     0.0         1.0  102.0     Independent   1000000.0   42446.0  0.0  0.0  10   \n",
       "1  Independent          0        2     0.0         1.0  202.0     Independent   1000000.0  151421.0  0.0  0.0  31   \n",
       "2    Franchise          0        4     0.0         1.0  248.0    Crowne Plaza  24000000.0  142978.0  0.0  0.0  39   \n",
       "\n",
       "                       tract  year  log1_landsf  log1_age  log1_largestmeetingspace  log_saleprice  log_sizesf  \\\n",
       "0      Dothan/Enterprise, AL  2009    11.777610  3.891820                  7.601402      13.815511   10.655988   \n",
       "1         Alabama North Area  2010    12.798153  3.401197                  8.006701      13.815511   11.927819   \n",
       "2  Black Canyon Corridor, AZ  2007    12.787249  3.295837                  8.594339      16.993564   11.870446   \n",
       "\n",
       "   log_rooms  log_floors   year_2      year_3   age_2     age_3  floors_2  land  selfrun  \n",
       "0   4.624973    0.693147  4036081  8108486729  2304.0  110592.0       4.0     1        0  \n",
       "1   5.308268    1.098612  4040100  8120601000   841.0   24389.0       9.0     1        0  \n",
       "2   5.513429    1.386294  4028049  8084294343   676.0   17576.0      16.0     1        0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect data\n",
    "all_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we need to drop the first column, which probably was a (meaningless) index that mistakenly was made a column when importing the data. Also, I will drop the binary variable \"resort\", since resort is also a category of the variable \"location\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.drop(['Unnamed: 0', 'resort', 'land'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "Let's now start with the actual preprocessing. As mentioned in the introduction and problem statement, an important goal of this project is to show that practitioners influenced predominantly by applied statistics should adopt common practices from machine learning to validate models. (While these approaches may have originated in statistics, the point is that they have only become common practice in machine learning.)\n",
    "\n",
    "The first such technique is the idea of withholding data (called a test set) when training the model, and then using these withheld data to assess performance. There are two main approaches of how to divide the data into training and test set: We can either split the data randomly, or we can choose a point in time and use all the data originating from an earlier point in time as the training set, and use the later data as the test set.  The former approach works best for independent data, and the latter works best for time-series data.\n",
    "\n",
    "Our data fall somewhere in between: They are not fully independent, because observations that are closer in time are likely to be more similar, even if we take into account all available predictors. (This is often described as having correlated errors.)  On the other hand, though, we are not dealing with a true time-series, because we are not following the same units over time. (An example of a pooled time-series would be repeated sales data.)  In this true time-series case, we can commonly observe such an extreme serial correlation of errors that this autoregressive process contains more useful information for prediction than the actual features do.\n",
    "\n",
    "I decided on a compromise: For the test set, I use the latest approximately 20% of the data, which cover the last 1.5 years of the sample. This should make sure we are not overly optimistic, as this makes our calculation of the test accuracy more similar to making predictions for use in practice, when we will probably not have a good estimate of the current year's average price yet. (When I calculate the predictions, I simply treat them as if they were from the last year from which we had data, which should give a better estimate than leaving out the information for year completely, because presumably this year's price will be more similar to last year's prize than to the average price of the last  25 years.)\n",
    "\n",
    "A disadvantage of this strategy is that the newest portion of the data may not be representative of all years, for example because it is drawn only from a particular point in the business cycle.  However, this is likely not a severe problem for our case, because the business cycle presumably mainly influences the level of the price (captured, in the case of a linear regression model, by the intercept) while the price determinants should stay pretty stable otherwise. To be on the safe side, I do split the data randomly when creating a validation set from the training set when performing cross-validation.  The reason that I pick a different strategy here is that we are not interested in the absolute level of the validation error here, only the relative performance of the different hyperparameters.  As a result, it's not a problem that random splitting causes an optimistic bias, because this should affect all hyperparameters values equally.\n",
    "\n",
    "Let's now go ahead and see which cut-off we should choose in order to get a test set of about 20% (around 1500 observations). To do so, we group the data by year and count the number of observations per year.  Then we compute the cumulative sum, going backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cumulative Sum (counting backwards)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>1917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>2756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>3497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cumulative Sum (counting backwards)\n",
       "year                                     \n",
       "2017                                  601\n",
       "2016                                 1917\n",
       "2015                                 2751\n",
       "2014                                 2756\n",
       "2013                                 3497"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Train-test split\n",
    "# =================\n",
    "\n",
    "# Count number of observations by year\n",
    "year_counts_cum = all_data.loc[:, [\"year\", \"id\"]] \\\n",
    "    .groupby(\"year\") \\\n",
    "    .count() \\\n",
    "    .sort_index(ascending=False) \\\n",
    "    .cumsum() \n",
    "\n",
    "# Properly name counts\n",
    "year_counts_cum = year_counts_cum. \\\n",
    "    rename(columns={'id': 'Cumulative Sum (counting backwards)'})\n",
    "\n",
    "# Print the last 5 years\n",
    "year_counts_cum.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that, if the data is relatively evenly distributed and 2016, we should cut in the middle of that year.  Thus, let's put all observations that fall in the third quarter 2016 or later into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in test set: 601\n"
     ]
    }
   ],
   "source": [
    "# Index for test set\n",
    "test_index = (all_data.loc[\n",
    "    (all_data.year > 2016) ]\n",
    "#     | ((all_data.year == 2016) & (all_data.quarter >= 3))] \\\n",
    "    .index\n",
    ")              \n",
    "# Index for training set \n",
    "train_index = all_data.index.difference(test_index)\n",
    "\n",
    "# Check length\n",
    "print(f'Number of observations in test set: {len(test_index)}')\n",
    "\n",
    "# Get test and train set\n",
    "data_test = all_data.loc[all_data.index.isin(test_index)]\n",
    "data_train = all_data.loc[all_data.index.isin(train_index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us the test set of about 1300 observations.  Let's check the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check results\n",
    "# assert data_train.year.max() == 2016\n",
    "# assert data_test.year.min() == 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since linear and tree-based models require different preprocessing, we will save another copy to use in another notebook to build a linear model for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data to use with linear model\n",
    "# joblib.dump(data_train, '../data/data_train_lin')\n",
    "# joblib.dump(data_test, '../data/data_test_lin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_lin = data_train\n",
    "data_test_lin = data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1bb71\\Anaconda3\\envs\\hotels2\\lib\\site-packages\\pandas\\core\\generic.py:5209: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "# Set year for test set to the most recent year\n",
    "data_test.year = 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpooled\n",
    "#### All logs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will perform one-hot encoding for categorical variables. By contrast to the previous notebook, we will also treat the ordinal variable, hotel category, as categorical. This is necessary because linear models would otherwise wrongly treat the variable as measured on an interval-scale, which is unlikely to be appropriate (this would assume that the difference between each succeeding pair of categories is identical in size).\n",
    "\n",
    "For the models without regularization, we need to drop the first category of each categorical variable in order to avoid collinearity. This is not so easy to achieve with the python data science stack, because we normally would not want to estimate a model without regularization. Pandas.get_dummies() does offer this possibility, but it then does not give us the option to transform the training data using the same encoding. Thus, the best option is to use scikit-learn's one-hot encoder on each categorical column individually. I will store these in a list, and then drop the first column before concatenating these new data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['msa', 'tract', 'location', 'category',\n",
    "        'saleaffiliation', 'operation', 'year', 'quarter']\n",
    "# Instantiate one-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "# Create lists to store encodings for each variable\n",
    "X_train_cats = []\n",
    "X_test_cats = []\n",
    "feature_names_cats = []\n",
    "# Iterate through categorical variables\n",
    "for cat in cats:\n",
    "    # Encode variable i for TEST data\n",
    "    ohe_train_i = ohe.fit_transform(\n",
    "                data_train.loc[:, [cat]])\n",
    "    # Encode variable i for TRAIN data\n",
    "    ohe_test_i = ohe.transform(\n",
    "                data_test.loc[:, [cat]])\n",
    "    \n",
    "    # Discard first column and save\n",
    "    X_train_cats.append(ohe_train_i[:,1:])\n",
    "    X_test_cats.append(ohe_test_i[:,1:])\n",
    "    \n",
    "    # Save feature names (w/o first column)\n",
    "    feature_names_cats.extend(\n",
    "        ohe.get_feature_names()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables excluded:  ['year_2', 'year_3', 'age_2', 'age_3', 'floors_2', 'landsf', 'age', 'largestmeetingspace', 'sizesf', 'rooms', 'floors', 'saleprice', 'log_saleprice', 'id', 'new']\n"
     ]
    }
   ],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "        data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and train the model.  Even though I removed the first category of each categorical variable for which I used one-hot-encoding, I also had to remove the intercept in order to get a reasonable prediction on the test set. (With the intercept, I got a negative R^2.)  Even though statisticians have developed tools to dig deeper into collinearity to find out which variables are causing the problem, I don't waste any time on this. As explained above, there is no reason to estimate a model without regularization anyway, except for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr= LinearRegression(fit_intercept=True)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row0\" class=\"row_heading level0 row0\" >allsuites</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row0_col0\" class=\"data row0 col0\" >0.00941</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row1\" class=\"row_heading level0 row1\" >boutique</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row1_col0\" class=\"data row1 col0\" >-0.01180</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row2\" class=\"row_heading level0 row2\" >casino</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row2_col0\" class=\"data row2 col0\" >0.88051</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row3\" class=\"row_heading level0 row3\" >cbd</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row3_col0\" class=\"data row3 col0\" >512181862717.14740</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row4\" class=\"row_heading level0 row4\" >conference</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row4_col0\" class=\"data row4 col0\" >0.25458</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row5\" class=\"row_heading level0 row5\" >convention</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row5_col0\" class=\"data row5 col0\" >0.32425</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row6\" class=\"row_heading level0 row6\" >golf</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row6_col0\" class=\"data row6 col0\" >0.14555</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row7\" class=\"row_heading level0 row7\" >indoorcorridors</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row7_col0\" class=\"data row7 col0\" >0.02813</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row8\" class=\"row_heading level0 row8\" >log1_age</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row8_col0\" class=\"data row8 col0\" >-0.11754</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row9\" class=\"row_heading level0 row9\" >log1_landsf</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row9_col0\" class=\"data row9 col0\" >-0.00131</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row10\" class=\"row_heading level0 row10\" >log1_largestmeetingspace</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row10_col0\" class=\"data row10 col0\" >0.00232</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row11\" class=\"row_heading level0 row11\" >log_floors</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row11_col0\" class=\"data row11 col0\" >0.11282</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row12\" class=\"row_heading level0 row12\" >log_rooms</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row12_col0\" class=\"data row12 col0\" >0.45760</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row13\" class=\"row_heading level0 row13\" >log_sizesf</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row13_col0\" class=\"data row13 col0\" >0.15507</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row14\" class=\"row_heading level0 row14\" >multiproperty</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row14_col0\" class=\"data row14 col0\" >-0.05150</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row15\" class=\"row_heading level0 row15\" >portfolio</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row15_col0\" class=\"data row15 col0\" >0.20621</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row16\" class=\"row_heading level0 row16\" >restaurant</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row16_col0\" class=\"data row16 col0\" >0.01472</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row17\" class=\"row_heading level0 row17\" >selfrun</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row17_col0\" class=\"data row17 col0\" >-0.07603</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row18\" class=\"row_heading level0 row18\" >ski</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row18_col0\" class=\"data row18 col0\" >0.15659</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9level0_row19\" class=\"row_heading level0 row19\" >spa</th>\n",
       "                        <td id=\"T_8c175726_032e_11ea_9b06_5c879cd5d5e9row19_col0\" class=\"data row19 col0\" >0.17430</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x229874e26a0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8640254134000869"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8.395883920329746e+19"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's wait with interpreting these results until the next notebook, where we can compare the results from all models.  Thus, we will store these results for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8640254134000869, -8.395883920329746e+19]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_up = [r2_lr_train, r2_lr]\n",
    "r2_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# results without intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr2 = LinearRegression(fit_intercept=False)\n",
    "lr2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.859678710446465"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr2.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5791522202341939"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr2.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6949, 1216)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-7ef4fe3d2683>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise Exception ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logs or squares\n",
    "Now let's estimate the same type of model but with different predictors: Instead of using all logs for numeric variables, I will follow Das et al (2017) and instead use a square term for age and floors, as well as a the levels (rather than log) for the amount of land included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2include = \\\n",
    "        ['allsuites', 'boutique', 'casino', 'cbd', \n",
    "         'conference', 'convention', 'golf', 'indoorcorridors',\n",
    "         'multiproperty', 'portfolio', 'restaurant', \n",
    "         'ski', 'spa', 'landsf', 'age', 'age_2',\n",
    "         'log1_largestmeetingspace', 'log_sizesf', 'log_rooms', \n",
    "         'floors', 'floors_2', 'selfrun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.loc[:, v2include]\n",
    "X_test_rest= data_test.loc[:, v2include]\n",
    "\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr= LinearRegression(fit_intercept=False)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, let's print the results from the model using all logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 for model with all logs \n",
    "# (for training and test set, respectively)\n",
    "r2_up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the two models performed very similarly, especially on the training set. On the test set, the model with polynomial terms performs slightly better than the model with all logs.  However, the difference is likely within the margin of error. (Indeed, when I tried a different specification that used a linear time trend, I got the reverse result.)  Thus, I will stick with the model with all logs for now, since this is more robust to outliers: As already mentioned, polynomials can give predictions that are far off for observations that have variables with extreme values, such as very old buildings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to estimate the pooled model without regularization.  To do so, we first repeat the preprocessing steps but do not perform one-hot encoding for the three categorical variables that have a large number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['location', 'category', 'operation', \n",
    "        'year', 'quarter']\n",
    "# Categorical variables to exclude\n",
    "cats_out = ['msa', 'tract', 'saleaffiliation']\n",
    "# Instantiate one-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "# Create lists to store encodings for each variable\n",
    "X_train_cats = []\n",
    "X_test_cats = []\n",
    "feature_names_cats = []\n",
    "for cat in cats:\n",
    "    # Encode variable i for TEST data\n",
    "    ohe_train_i = ohe.fit_transform(\n",
    "                data_train.loc[:, [cat]])\n",
    "    # Encode variable i for TRAIN data\n",
    "    ohe_test_i = ohe.transform(\n",
    "                data_test.loc[:, [cat]])\n",
    "    \n",
    "    # Discard first column and save\n",
    "    X_train_cats.append(ohe_train_i[:,1:])\n",
    "    X_test_cats.append(ohe_test_i[:,1:])\n",
    "    \n",
    "    # Save feature names\n",
    "    feature_names_cats.extend(\n",
    "        ohe.get_feature_names()[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "        data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + cats_out + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + cats_out + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train= np.concatenate([X_train_rest] + X_train_cats, \n",
    "                         axis=1)\n",
    "X_test= np.concatenate([X_test_rest] + X_test_cats, \n",
    "                         axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + feature_names_cats\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready again to estimate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X_train, y_train)\n",
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    lr.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the explained variance for training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_lr= lr.predict(X_train)\n",
    "r2_lr_train = r2_score(y_train, y_lr)\n",
    "r2_lr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TEST set\n",
    "y_lr= lr.predict(X_test)\n",
    "r2_lr= r2_score(y_test, y_lr)\n",
    "r2_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance\n",
    "r2_p = [r2_lr_train, r2_lr]\n",
    "r2_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the model does a little better on the test set. On average, we would expect it to do worse. However, there is variance to our estimated performance on training and test set.  In our case, this variance is increased by the fact that we don't have a huge sample size (the test set contains about 1300 observations), and that we are using a single validation set rather than cross-validation in order to take into account the time-series nature of the data. Even more importantly, the training and test set do not come from the same distribution, because we split the data based on time.  It seems that the newer data were easier to predict, maybe because we have more data for the intercept of year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model with elastic net regularization\n",
    "### Unpooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the preprocessing steps again.  By contrast to the models without regularization, we do not get rid of the first category for each categorical variables.  The reason this is not necessary is that regularization is an alternative way to avoid collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "cats = ['msa', 'tract', 'location', 'category',\n",
    "        'saleaffiliation', 'operation', 'year',\n",
    "        'quarter']\n",
    "# Perform one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "X_train_cats = ohe.fit_transform(\n",
    "    data_train.loc[:, cats])        \n",
    "X_test_cats = ohe.transform(\n",
    "    data_test.loc[:, cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "    data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats + v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train_up = np.concatenate([X_train_rest, X_train_cats], \n",
    "                         axis=1)\n",
    "X_test_up = np.concatenate([X_test_rest, X_test_cats], \n",
    "                         axis=1)\n",
    "# X_train_up = X_train_up.drop(\n",
    "#     v2exclude, axis=1)\n",
    "# X_test_up = X_test_up.drop(\n",
    "#     v2exclude, axis=1)\n",
    "\n",
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + list(ohe.get_feature_names())\n",
    "\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can no go ahead and train the model.  Like for gradient boosting, will use Hyperopt to perform Bayesian hyperparameters optimization.  It probably does not lead to any big increase in performance or decrease in computational cost compared to a brute-force search, because computational costs for a linear model are relatively low anyways for a data set of our size. But since it is not too much of a burden to adapt the  code from XGBoost, we might as well reuse it just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to carry out hyperparameter optimization\n",
    "def find_best_hp(LEARNER, space, model_name, \n",
    "                 X_train, y_train, \n",
    "                 adjust_params=None,n_folds=5, n_jobs=-1, max_evals=20):\n",
    "    \"\"\"Find best hyperparameters for a given regressor and search space.\"\"\"\n",
    "    \n",
    "    # Trials object to track progress (not currently used)\n",
    "    trials = Trials()\n",
    "\n",
    "    # CSV file to track progress\n",
    "    progress_file_path = '../progress_' + model_name + '.csv'\n",
    "    with open(progress_file_path, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header to the file\n",
    "        writer.writerow(['loss', 'params'])\n",
    "\n",
    "    # Objective function to minimize\n",
    "    def objective(params, LEARNER=LEARNER, \n",
    "                  progress_file_path=progress_file_path,\n",
    "                  n_folds=n_folds, n_jobs=n_jobs):\n",
    "        \"\"\"Objective function to minimize\"\"\"\n",
    "        \n",
    "        # Adjust parameters, if specified\n",
    "        if adjust_params is not None:\n",
    "            params = adjust_params(params)\n",
    "    \n",
    "        # Instantiate LEARNER\n",
    "        learner = LEARNER(**params)\n",
    "        \n",
    "        ## Generate indices for cross-validation\n",
    "        # If only one \"fold\" is desired, split into train and validation set\n",
    "        if n_folds == 1: \n",
    "            cv = ShuffleSplit(n_splits=1, test_size=.2, \n",
    "                                        random_state=1)\n",
    "        # Otherwise, generate indices for proper cross-validation split\n",
    "        else:  \n",
    "            cv = KFold(n_folds, random_state=1)\n",
    "\n",
    "        # Compute average precision through CV / validation set\n",
    "        score = cross_val_score(learner, X_train, y_train, cv=cv,\n",
    "                                scoring='r2', n_jobs=n_jobs)\n",
    "        # Compute loss as the negative mean of the average precision scores\n",
    "        # (since hyperopt can only minimize a function)\n",
    "        loss = -score.mean()\n",
    "        \n",
    "        # Save results to csv file\n",
    "        with open(progress_file_path, 'a') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([loss, params])\n",
    "        \n",
    "        # Return results\n",
    "        return {'loss': loss, 'params': params, 'status': STATUS_OK}\n",
    "    \n",
    "    # Minimize objective\n",
    "    best = fmin(objective, space, algo=tpe.suggest,\n",
    "                max_evals=max_evals, trials=trials)\n",
    "\n",
    "    # Get the values of the optimal parameters\n",
    "    best_params = space_eval(space, best)\n",
    "    # Adjust best parameters, if specified\n",
    "    if adjust_params is not None:\n",
    "        best_params = adjust_params(best_params)\n",
    "\n",
    "    # Re-fit the model with the optimal hyperparamters\n",
    "    learner = LEARNER(**best_params)\n",
    "    learner.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model to disk\n",
    "    joblib.dump(learner, '../saved_models/' + model_name + '.joblib')\n",
    "    \n",
    "    # Print best parameters\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50\n",
    "N_FOLDS = 20\n",
    "\n",
    "# Search space (also includes constant parameters)\n",
    "space = {\n",
    "    'max_iter': 1000,\n",
    "    'tol':1E-3,\n",
    "    'random_state': 1,\n",
    "    'alpha': hp.lognormal('alpha', np.log(1E-4), 4),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters (defined above)\n",
    "find_best_hp(ElasticNet, space, model_name='el_up',\n",
    "              X_train=X_train_up, y_train=y_train,\n",
    "              max_evals=MAX_EVALS, n_jobs=3, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model, if necessary\n",
    "el_up = joblib.load('../saved_models/el_up.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full results from progress file\n",
    "el_up_results = pd.read_csv('../progress_el_up.csv')\n",
    "\n",
    "# Get validation score\n",
    "# Extract AP for each iteration\n",
    "r2_el_up = - el_up_results.loss\n",
    "\n",
    "# Plot r2 per iteration\n",
    "r2_el_up.plot()\n",
    "plt.title('Performance on test set')\n",
    "plt.ylabel('R_2')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals that optimizing hyperparameters to regularize a linear model is relatively simple, so performance seems pretty stable without much improvement for further iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_el_up = el_up.predict(X_train_up)\n",
    "r2_el_up_train = r2_score(y_train, y_el_up)\n",
    "r2_el_up_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TEST set\n",
    "y_el_up = el_up.predict(X_test_up)\n",
    "r2_el_up = r2_score(y_test, y_el_up)\n",
    "r2_el_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance\n",
    "r2_up_r = [r2_el_up_train, r2_el_up]\n",
    "r2_up_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the explained variance reveals that regularization gave us a boost in performance.  However, as mentioned above, we will save a comparison for a final notebook.\n",
    "\n",
    "Let's print the coefficients, ignoring the intercepts for all categorical variables in order to limit the size of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    el_up.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooled\n",
    "Finally, we will re-estimate the model for the pooled case. Again, we do not drop the first category when performing one-hot encoding, since regularization effectively deals with collinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables to include\n",
    "cats = ['location', 'category', 'operation', \n",
    "        'year', 'quarter']\n",
    "# Categorical variables to exclude\n",
    "cats_out = ['msa', 'tract', 'saleaffiliation']\n",
    "# Perform one-hot encoding\n",
    "ohe = OneHotEncoder(handle_unknown='ignore',\n",
    "                    sparse=False)\n",
    "X_train_cats = ohe.fit_transform(\n",
    "    data_train.loc[:, cats])        \n",
    "X_test_cats = ohe.transform(\n",
    "    data_test.loc[:, cats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of variables to exclude from X\n",
    "v2exclude= list(\n",
    "    data_train.columns[\n",
    "    data_train.columns.str.contains(r'_[23]')]) # Polynomials\n",
    "v2exclude.extend(\n",
    "    ['landsf', 'age', 'largestmeetingspace', 'sizesf', # Levels\n",
    "     'rooms','floors'])\n",
    "v2exclude.extend(\n",
    "    ['saleprice', 'log_saleprice', 'id', 'new'])\n",
    "# Print to make sure no variables are accidentally excluded\n",
    "print('Variables excluded: ', v2exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-categorical columns for X\n",
    "X_train_rest= data_train.drop(cats  + cats_out+ v2exclude,\n",
    "                               axis=1)\n",
    "X_test_rest= data_test.drop(cats + cats_out+ v2exclude,\n",
    "                             axis=1)\n",
    "# Merge categorical and non-categorical variables\n",
    "X_train_p = np.concatenate([X_train_rest, X_train_cats], \n",
    "                         axis=1)\n",
    "X_test_p = np.concatenate([X_test_rest, X_test_cats], \n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names\n",
    "feature_names= list(X_train_rest.columns) \\\n",
    "    + list(ohe.get_feature_names())\n",
    "\n",
    "# Create target variables\n",
    "y_train = data_train.loc[:, 'log_saleprice']\n",
    "y_test = data_test.loc[:, 'log_saleprice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVALS = 50\n",
    "N_FOLDS = 20\n",
    "\n",
    "# Search space (also includes constant parameters)\n",
    "space = {\n",
    "    'max_iter': 1000,\n",
    "    'tol':1E-3,\n",
    "    'random_state': 1,\n",
    "    'alpha': hp.lognormal('alpha', np.log(1E-4), 4),\n",
    "    'l1_ratio': hp.uniform('l1_ratio', 0, 1)\n",
    "}\n",
    "\n",
    "# Find best hyperparameters (defined above)\n",
    "find_best_hp(ElasticNet, space, model_name='el_p',\n",
    "              X_train=X_train_p, y_train=y_train,\n",
    "              max_evals=MAX_EVALS, n_jobs=3, n_folds=N_FOLDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model, if necessary\n",
    "el_p = joblib.load('../saved_models/el_p.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full results from progress file\n",
    "el_results = pd.read_csv('../progress_el_p.csv')\n",
    "\n",
    "# Get validation score\n",
    "# Extract AP for each iteration\n",
    "r2_el_p = - el_results.loss\n",
    "\n",
    "# Plot r2 per iteration\n",
    "r2_el_p.plot()\n",
    "plt.title('Performance on test set')\n",
    "plt.ylabel('r2')\n",
    "plt.xlabel('Iteration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining how the performance change over the iterations of the hyperparameters optimization, we see that it stayed almost stable.  This is due to the fact that hyperparameters optimization for this case is easy (compared, e.g., what we saw for gradient boosting, where we optimized over a higher-dimensional space.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TRAINING set\n",
    "y_el_p = el_p.predict(X_train_p)\n",
    "r2_el_p_train = r2_score(y_train, y_el_p)\n",
    "r2_el_p_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_2 on TEST set\n",
    "y_el_p = el_p.predict(X_test_p)\n",
    "r2_el_p = r2_score(y_test, y_el_p)\n",
    "r2_el_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance\n",
    "r2_p_r = [r2_el_p_train, r2_el_p]\n",
    "r2_p_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sorted coefficients with feature names\n",
    "coef= pd.Series(\n",
    "    el_p.coef_,\n",
    "    index=feature_names) \\\n",
    "    .sort_values(ascending=False)\n",
    "\n",
    "# Print results for non-categorical variables\n",
    "pd.DataFrame(coef \n",
    "    .loc[~ coef.index.str.startswith('x')]) \\\n",
    "    .sort_index() \\\n",
    "    .style.format('{:.5F}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save performance\n",
    "Finally, we concatenate the performance of all linear models into a data frame and save it to disk. We will load this in the final notebook, where we compare the performance of these models with each other and with the performance of gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all R_2 scores from OLS\n",
    "r2_lin = pd.DataFrame([r2_up, r2_p, r2_up_r, r2_p_r],\n",
    "             columns=['Train', 'Test'],\n",
    "             index=['OLS, unpooled, no reg.',\n",
    "                    'OLS, pooled, no reg.',\n",
    "                    'linear regression, unpooled, elastic net reg.',\n",
    "                    'linear regression, pooled, elastic net reg.']\n",
    ")\n",
    "joblib.dump(r2_lin, '../saved_models/r2_lin.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('jupyter nbconvert --to html 2_modeling_linear.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "253.764px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
